#!/usr/bin/env python3
"""
äº¤äº’å¼æŒ‰é’®æ£€æµ‹å™¨ - ROS2 AprilTagç‰ˆæœ¬
ä½¿ç”¨ pyrealsense2 ç›´æ¥è¯»å–ç›¸æœºï¼ˆé«˜æ€§èƒ½ï¼‰+ ROS2 å‘å¸ƒç»“æœ
é›†æˆAprilTagå®æ—¶æ£€æµ‹ï¼Œå‘å¸ƒå¤¹çˆªåæ ‡ç³»æ¬§æ‹‰è§’
åŒæ­¥YOLOæ£€æµ‹ï¼Œå®æ—¶æ€§ä¼˜åŒ–
"""
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PointStamped, Vector3
from std_msgs.msg import String
from visualization_msgs.msg import Marker
import cv2
from ultralytics import YOLO
import numpy as np
import pyrealsense2 as rs
import time
import math

# å¯¼å…¥åæ ‡è½¬æ¢æ‰€éœ€çš„æ¨¡å—
from piper_sdk import C_PiperInterface_V2
from piper_arm import PiperArm

# å¯¼å…¥AprilTagæ£€æµ‹å™¨
from dt_apriltags import Detector

PI = math.pi

# ========================================
# æ€§èƒ½è°ƒä¼˜å‚æ•°
# ========================================
DETECTION_SKIP_FRAMES = 0  # ğŸ”§ å¼‚æ­¥æ¨¡å¼ï¼šæ¯å¸§éƒ½æ”¾å…¥é˜Ÿåˆ—ï¼Œæ£€æµ‹çº¿ç¨‹è‡ªåŠ¨å¤„ç†æœ€æ–°å¸§
YOLO_CONF_THRESHOLD = 0.4  # ğŸ”§ é™ä½é˜ˆå€¼æé«˜å¬å›ç‡ï¼ˆå°å›¾åƒéœ€è¦ï¼‰
YOLO_SCALE_FACTOR = 0.1    # ğŸ”§ ğŸš€ æé™æ¨¡å¼ï¼š640x480 â†’ 64x48 (100å€åŠ é€Ÿ!)
UI_REFRESH_RATE = 30       # ğŸ”§ UIåˆ·æ–°ç‡ï¼ˆHzï¼‰ï¼Œç‹¬ç«‹äºæ£€æµ‹é¢‘ç‡

# ========================================
# å…¨å±€å˜é‡
# ========================================
all_detections = []
selected_button_index = -1
selected_button_locked = False
selected_box_signature = None

# é¼ æ ‡ä½ç½®
mouse_x, mouse_y = 0, 0

# å½“å‰å¸§æ•°æ®
current_depth_data = None
current_color_data = None
current_depth_intrin = None

# äº¤äº’æ§åˆ¶
is_paused = False
paused_frame = None
paused_detections = []

# å¸§è®¡æ•°å™¨
frame_counter = 0

# FPSç»Ÿè®¡
last_fps_time = time.time()
fps_counter = 0
current_fps = 0.0
last_detect_time_ms = 0.0  # æœ€è¿‘ä¸€æ¬¡æ£€æµ‹è€—æ—¶




# ========================================
# é¼ æ ‡å›è°ƒå‡½æ•°
# ========================================
def mouse_callback(event, x, y, flags, param):
    """å¤„ç†é¼ æ ‡äº‹ä»¶ï¼Œå…è®¸ç”¨æˆ·ç‚¹å‡»é€‰æ‹©æŒ‰é’®"""
    global selected_button_index, mouse_x, mouse_y, all_detections
    global current_depth_data, current_color_data, current_depth_intrin
    global selected_button_locked, is_paused, paused_detections
    global selected_box_signature
    
    mouse_x, mouse_y = x, y
    
    if event == cv2.EVENT_LBUTTONDOWN:
        print(f"\n{'='*70}")
        print(f"[é¼ æ ‡ç‚¹å‡»] ä½ç½®: ({x}, {y})")
        print(f"[æ£€æµ‹çŠ¶æ€] å½“å‰æ£€æµ‹åˆ° {len(all_detections)} ä¸ªæŒ‰é’®")
        
        detections_to_check = paused_detections if is_paused else list(all_detections)
        
        found = False
        for idx, det in enumerate(detections_to_check):
            x1, y1, x2, y2, class_name, conf, center_3d = det
            
            print(f"  æ£€æŸ¥æŒ‰é’® #{idx}: ç±»å‹={class_name}, æ¡†=[{x1},{y1},{x2},{y2}]", end="")
            
            if x1 <= x <= x2 and y1 <= y <= y2:
                print(" â†’ âœ“ åŒ¹é…!")
                found = True
                selected_button_index = idx
                selected_button_locked = True
                
                # ç«‹å³åˆ·æ–°æ˜¾ç¤º
                if current_color_data is not None:
                    instant_display = visualize_detections(current_color_data, detections_to_check, selected_button_index)
                    cv2.imshow('detection', instant_display)
                    cv2.waitKey(1)
                
                if center_3d is None and current_depth_data is not None:
                    # ç®€åŒ–çš„3Dç‚¹æå–
                    cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)
                    depth = current_depth_data[cy, cx] * 0.001  # mm -> m
                    if depth > 0:
                        center_3d = np.array([
                            (cx - current_depth_intrin.ppx) * depth / current_depth_intrin.fx,
                            (cy - current_depth_intrin.ppy) * depth / current_depth_intrin.fy,
                            depth
                        ])
                    if is_paused:
                        paused_detections[idx] = (x1, y1, x2, y2, class_name, conf, center_3d)
                    if idx < len(all_detections):
                        all_detections[idx] = (x1, y1, x2, y2, class_name, conf, center_3d)
                
                remember_selected_detection(det)
                
                print(f"\n{'='*70}")
                print(f"âœ“âœ“âœ“ å·²é€‰æ‹©æŒ‰é’® #{idx} ã€å·²é”å®šã€‘âœ“âœ“âœ“")
                print(f"  ç±»å‹: {class_name}")
                print(f"  ç½®ä¿¡åº¦: {conf:.2f}")
                print(f"  æ£€æµ‹æ¡†: [{x1}, {y1}, {x2}, {y2}]")
                print(f"  2Dä¸­å¿ƒ: ({int((x1+x2)/2)}, {int((y1+y2)/2)})")
                if center_3d is not None:
                    print(f"  [ç›¸æœºåæ ‡ç³»] XYZ: ({center_3d[0]:.3f}, {center_3d[1]:.3f}, {center_3d[2]:.3f}) m")
                    
                    # è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
                    if param is not None and hasattr(param, 'piper_arm') and param.piper_arm is not None:
                        base_3d = transform_button_camera_to_base(center_3d, param.piper, param.piper_arm)
                        if base_3d is not None:
                            print(f"  [åŸºåº§åæ ‡ç³»] XYZ: ({base_3d[0]:.3f}, {base_3d[1]:.3f}, {base_3d[2]:.3f}) m")
                        else:
                            print(f"  [åŸºåº§åæ ‡ç³»] è½¬æ¢å¤±è´¥ï¼ˆæ— æ³•è·å–å…³èŠ‚è§’åº¦ï¼‰")
                    else:
                        print(f"  [åŸºåº§åæ ‡ç³»] N/A (Piper SDKæœªåˆå§‹åŒ–)")
                print(f"  æç¤º: æŒ‰ ENTER ç¡®è®¤ | æŒ‰ ESC å–æ¶ˆé€‰æ‹©")
                print(f"{'='*70}")
                break
            else:
                print(" â†’ âœ— ä¸åŒ¹é…")
        
        if not found:
            print(f"\n  âœ—âœ—âœ— ç‚¹å‡»ä½ç½® ({x}, {y}) ä¸åœ¨ä»»ä½•æŒ‰é’®å†…")
            print(f"{'='*70}")


# ========================================
# YOLO æ£€æµ‹
# ========================================
def YOLODetection(model, color_img, conf_threshold=0.5):
    """YOLO æ£€æµ‹ - å›ºå®šç¼–å·é¡ºåº"""
    results = model(color_img, verbose=False)
    target_boxes = []
    
    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            conf = float(box.conf[0])
            cls_id = int(box.cls[0])
            class_name = model.names[cls_id]
            
            if conf >= conf_threshold:
                target_boxes.append((x1, y1, x2, y2, class_name, conf))
    
    target_boxes.sort(key=lambda box: (box[1] // 100, box[0]))
    return target_boxes


def _make_signature_from_detection(det):
    """æ ¹æ®æ£€æµ‹æ¡†ç”Ÿæˆå”¯ä¸€ç­¾å"""
    x1, y1, x2, y2, class_name, *_ = det
    center = ((x1 + x2) / 2.0, (y1 + y2) / 2.0)
    return {"class": class_name, "center": center, "bbox": (x1, y1, x2, y2)}


def remember_selected_detection(det):
    """è®°å½•å½“å‰é€‰ä¸­çš„æ£€æµ‹æ¡†ç‰¹å¾"""
    global selected_box_signature
    selected_box_signature = _make_signature_from_detection(det)


def sync_selection_with_detections():
    """åœ¨æ£€æµ‹ç»“æœæ›´æ–°åï¼Œé‡æ–°å…³è”å·²é€‰ä¸­çš„æŒ‰é’®"""
    global selected_button_index, selected_box_signature

    if selected_box_signature is None or not all_detections:
        return

    best_idx = -1
    best_dist = float('inf')
    target_class = selected_box_signature["class"]
    target_center = selected_box_signature["center"]

    for idx, det in enumerate(all_detections):
        x1, y1, x2, y2, class_name, *_ = det
        if class_name != target_class:
            continue
        center = ((x1 + x2) / 2.0, (y1 + y2) / 2.0)
        dist = np.linalg.norm(np.array(center) - np.array(target_center))
        if dist < best_dist:
            best_dist = dist
            best_idx = idx

    if best_idx != -1 and best_dist < 80:
        selected_button_index = best_idx
    else:
        selected_button_index = -1
        selected_box_signature = None


# ========================================
# åæ ‡è½¬æ¢ï¼šç›¸æœº â†’ åŸºåº§ï¼ˆæŒ‰é’®ä¸“ç”¨ï¼‰
# ========================================
def transform_button_camera_to_base(button_camera, piper, piper_arm):
    """
    å°†ç›¸æœºåæ ‡ç³»çš„æŒ‰é’®ä½ç½®è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
    
    Args:
        button_camera: [x, y, z] æˆ– np.array ç›¸æœºåæ ‡ç³»
        piper: Piper SDKå®ä¾‹
        piper_arm: PiperArmå®ä¾‹
    
    Returns:
        button_base: [x, y, z] åŸºåº§åæ ‡ç³»ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        from utils.utils_math import quaternion_to_rotation_matrix
        
        # è·å–å½“å‰å…³èŠ‚è§’åº¦
        msg = piper.GetArmJointMsgs()
        current_joints = [
            msg.joint_state.joint_1 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_2 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_3 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_4 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_5 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_6 * 1e-3 * PI / 180.0,
        ]
        
        # åŸºåº§åˆ°link6çš„å˜æ¢ï¼ˆæ­£å‘è¿åŠ¨å­¦ï¼‰
        base_T_link6 = piper_arm.forward_kinematics(current_joints)
        
        # link6åˆ°ç›¸æœºçš„å˜æ¢ï¼ˆæ‰‹çœ¼æ ‡å®šï¼‰
        link6_T_cam = np.eye(4)
        link6_T_cam[:3, :3] = quaternion_to_rotation_matrix(piper_arm.link6_q_camera)
        link6_T_cam[:3, 3] = piper_arm.link6_t_camera
        
        # æŒ‰é’®åœ¨ç›¸æœºåæ ‡ç³»çš„é½æ¬¡åæ ‡
        button_cam_h = np.array([button_camera[0], button_camera[1], button_camera[2], 1.0])
        
        # è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
        button_base = base_T_link6 @ link6_T_cam @ button_cam_h
        
        return button_base[:3]
    except Exception as e:
        return None





# ========================================
# å¯è§†åŒ–
# ========================================
def visualize_detections(color_img, detections, selected_idx, apriltag_pose_info=None):
    """å¯è§†åŒ–æ£€æµ‹ç»“æœï¼ˆåŒ…å«AprilTagä½å§¿ä¿¡æ¯ï¼‰"""
    annotated = color_img.copy()
    
    for idx, det in enumerate(detections):
        x1, y1, x2, y2, class_name, conf, center_3d = det
        
        if idx == selected_idx:
            color = (0, 255, 0)
            thickness = 4
        else:
            color = (255, 0, 0)
            thickness = 2
        
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, thickness)
        
        label = f"#{idx} {class_name} {conf:.2f}"
        cv2.putText(annotated, label, (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        if center_3d is not None:
            coord_text = f"({center_3d[0]:.2f}, {center_3d[1]:.2f}, {center_3d[2]:.2f})"
            cv2.putText(annotated, coord_text, (x1, y2 + 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
    
    # FPSä¿¡æ¯
    global current_fps
    cv2.putText(annotated, f"FPS:{current_fps:.0f}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    # æ˜¾ç¤ºAprilTagä½å§¿ä¿¡æ¯ï¼ˆå·¦ä¾§ï¼‰
    if apriltag_pose_info is not None and len(apriltag_pose_info) > 0:
        y_offset = 60  # ä»FPSä¸‹æ–¹å¼€å§‹
        x_start = 10  # å·¦ä¾§èµ·å§‹ä½ç½®
        
        # è®¡ç®—éœ€è¦çš„èƒŒæ™¯é«˜åº¦ï¼ˆæ¯ä¸ªTagçº¦178è¡Œï¼ŒåŒ…å«æ–°å¢çš„Gripper in Baseï¼‰
        info_height = len(apriltag_pose_info) * 178
        
        # åŠé€æ˜èƒŒæ™¯
        overlay = annotated.copy()
        cv2.rectangle(overlay, (5, y_offset - 5), (430, y_offset + info_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, annotated, 0.4, 0, annotated)
        
        # æ ‡é¢˜
        cv2.putText(annotated, "=== AprilTag Pose ===", (x_start, y_offset),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255, 255, 0), 2)
        y_offset += 22
        
        for info in apriltag_pose_info:
            tag_id = info['tag_id']
            
            # Tag ID
            cv2.putText(annotated, f"--- Tag ID {tag_id} ---", (x_start, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 255), 2)
            y_offset += 18
            
            # AprilTagåœ¨ç›¸æœºåæ ‡ç³»
            cv2.putText(annotated, "[Tag in Camera]", (x_start, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 200, 100), 1)
            y_offset += 15
            
            x_cam = info['camera']['position'][0]
            y_cam = info['camera']['position'][1]
            z_cam = info['camera']['position'][2]
            cv2.putText(annotated, f"Pos: X{x_cam:+.3f} Y{y_cam:+.3f} Z{z_cam:+.3f}m", (x_start + 5, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)
            y_offset += 13
            
            roll = info['camera']['rpy'][0]
            pitch = info['camera']['rpy'][1]
            yaw = info['camera']['rpy'][2]
            cv2.putText(annotated, f"RPY: R{roll:+.1f} P{pitch:+.1f} Y{yaw:+.1f}deg", (x_start + 5, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255), 1)
            y_offset += 16
            
            # ç›¸æœºåæ ‡ç³»ï¼ˆå®é™…ä¸Šå’ŒTag Frameä¸€æ ·ï¼Œå¯ä»¥çœç•¥ï¼‰
            # cv2.putText(annotated, "[Camera]", (x_start, y_offset),
            #             cv2.FONT_HERSHEY_SIMPLEX, 0.4, (100, 200, 255), 1)
            # y_offset += 15
            
            # AprilTagåœ¨å¤¹çˆªåæ ‡ç³»
            cv2.putText(annotated, "[Tag in Gripper]", (x_start, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (100, 255, 100), 1)
            y_offset += 15
            
            x_grip = info['gripper']['position'][0]
            y_grip = info['gripper']['position'][1]
            z_grip = info['gripper']['position'][2]
            cv2.putText(annotated, f"Pos: X{x_grip:+.3f} Y{y_grip:+.3f} Z{z_grip:+.3f}m", (x_start + 5, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.35, (200, 255, 200), 1)
            y_offset += 13
            
            roll_grip = info['gripper']['rpy'][0]
            pitch_grip = info['gripper']['rpy'][1]
            yaw_grip = info['gripper']['rpy'][2]
            cv2.putText(annotated, f"RPY: R{roll_grip:+.1f} P{pitch_grip:+.1f} Y{yaw_grip:+.1f}deg", (x_start + 5, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.35, (200, 255, 200), 1)
            y_offset += 16
            
            # å½“å‰å¤¹çˆªå§¿æ€ï¼ˆåŸºåº§åæ ‡ç³»ï¼‰
            if info.get('gripper_in_base_rpy') is not None:
                cv2.putText(annotated, "[Current Gripper Pose]", (x_start, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 255, 255), 1)
                y_offset += 15
                
                roll_gb = info['gripper_in_base_rpy'][0]
                pitch_gb = info['gripper_in_base_rpy'][1]
                yaw_gb = info['gripper_in_base_rpy'][2]
                cv2.putText(annotated, f"RPY: R{roll_gb:+.1f} P{pitch_gb:+.1f} Y{yaw_gb:+.1f}deg (in Base)", (x_start + 5, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.35, (180, 255, 255), 1)
                y_offset += 18
            
            # AprilTagåœ¨åŸºåº§åæ ‡ç³»
            if info.get('base') is not None:
                cv2.putText(annotated, "[Tag in Base]", (x_start, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 150, 100), 1)
                y_offset += 15
                
                x_base = info['base']['position'][0]
                y_base = info['base']['position'][1]
                z_base = info['base']['position'][2]
                cv2.putText(annotated, f"Pos: X{x_base:+.3f} Y{y_base:+.3f} Z{z_base:+.3f}m", (x_start + 5, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 200, 150), 1)
                y_offset += 13
                
                roll_base = info['base']['rpy'][0]
                pitch_base = info['base']['rpy'][1]
                yaw_base = info['base']['rpy'][2]
                cv2.putText(annotated, f"RPY: R{roll_base:+.1f} P{pitch_base:+.1f} Y{yaw_base:+.1f}deg", (x_start + 5, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 200, 150), 1)
                y_offset += 18
            else:
                cv2.putText(annotated, "[Base] N/A", (x_start, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (100, 100, 100), 1)
                y_offset += 18
    
    return annotated


# ========================================
# ROS2 èŠ‚ç‚¹ï¼ˆä»…ç”¨äºå‘å¸ƒç»“æœï¼‰
# ========================================
class ButtonDetectorNode(Node):
    """æŒ‰é’®æ£€æµ‹èŠ‚ç‚¹ - ç›´æ¥è¯»å–ç›¸æœºç‰ˆæœ¬"""
    def __init__(self):
        super().__init__('button_detector_ros2_direct')
        self.get_logger().info("="*70)
        self.get_logger().info("äº¤äº’å¼æŒ‰é’®æ£€æµ‹å™¨ - ROS2 AprilTagç‰ˆæœ¬")
        self.get_logger().info("="*70)
        
        # åˆå§‹åŒ– Piper SDK
        try:
            self.piper = C_PiperInterface_V2("can0")
            self.piper.ConnectPort()
            self.piper_arm = PiperArm()
            self.get_logger().info("âœ“ Piper SDK åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.get_logger().warn(f"âš ï¸  Piper SDK åˆå§‹åŒ–å¤±è´¥: {e}")
            self.piper = None
            self.piper_arm = None
        
        # åŠ è½½ YOLO æ¨¡å‹
        self.model = YOLO('yolo_button.pt')
        self.get_logger().info("âœ“ YOLO æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºå‘å¸ƒå™¨
        self.point_pub = self.create_publisher(PointStamped, '/object_point', 10)  # ç›¸æœºç³»ï¼ˆå…¼å®¹ï¼‰
        self.point_base_pub = self.create_publisher(PointStamped, '/object_point_base', 10)  # åŸºåº§ç³»ï¼ˆæ–°å¢ï¼‰
        self.type_pub = self.create_publisher(String, '/button_type', 10)
        self.marker_pub = self.create_publisher(Marker, '/object_center_marker', 10)
        self.normal_pub = self.create_publisher(Vector3, '/button_normal', 10)  # å¤¹çˆªç³»æ¬§æ‹‰è§’ï¼ˆå…¼å®¹ï¼‰
        self.normal_base_pub = self.create_publisher(Vector3, '/button_normal_base', 10)  # åŸºåº§ç³»æ¬§æ‹‰è§’ï¼ˆæ–°å¢ï¼‰
        
        self.get_logger().info("âœ“ ROS2å‘å¸ƒå™¨å·²åˆ›å»º")
        self.get_logger().info("  - /object_point (ç›¸æœºç³»ï¼Œå…¼å®¹)")
        self.get_logger().info("  - /object_point_base (åŸºåº§ç³»ï¼Œæ¨è)")
        self.get_logger().info("  - /button_normal (å¤¹çˆªç³»ï¼Œå…¼å®¹)")
        self.get_logger().info("  - /button_normal_base (åŸºåº§ç³»ï¼Œæ¨è)")
        self.get_logger().info("="*70)
    
    def publish_result(self, center_3d, class_name):
        """å‘å¸ƒæ£€æµ‹ç»“æœåˆ°ROS2è¯é¢˜"""
        # å‘å¸ƒç‚¹åæ ‡
        point_msg = PointStamped()
        point_msg.header.stamp = self.get_clock().now().to_msg()
        point_msg.header.frame_id = "camera"
        point_msg.point.x = center_3d[0]
        point_msg.point.y = center_3d[1]
        point_msg.point.z = center_3d[2]
        self.point_pub.publish(point_msg)
        
        # å‘å¸ƒæŒ‰é’®ç±»å‹
        type_msg = String()
        type_msg.data = class_name
        self.type_pub.publish(type_msg)
        
        # å‘å¸ƒå¯è§†åŒ–Marker
        marker = Marker()
        marker.header.frame_id = "camera"
        marker.header.stamp = self.get_clock().now().to_msg()
        marker.ns = "button_center"
        marker.id = 0
        marker.type = Marker.SPHERE
        marker.action = Marker.ADD
        marker.pose.position.x = center_3d[0]
        marker.pose.position.y = center_3d[1]
        marker.pose.position.z = center_3d[2]
        marker.scale.x = 0.02
        marker.scale.y = 0.02
        marker.scale.z = 0.02
        marker.color.r = 1.0
        marker.color.g = 0.0
        marker.color.b = 0.0
        marker.color.a = 1.0
        self.marker_pub.publish(marker)


def main(args=None):
    rclpy.init(args=args)
    node = ButtonDetectorNode()

    # é…ç½® RealSenseï¼ˆå…ˆåˆå§‹åŒ–ä¸»ç¨‹åºçš„ç›¸æœºï¼‰
    pipeline = rs.pipeline()
    config = rs.config()
    
    # ğŸ” è‡ªåŠ¨æŸ¥è¯¢å¹¶é€‰æ‹©æœ€ä½³åˆ†è¾¨ç‡
    ctx = rs.context()
    devices = ctx.query_devices()
    if len(devices) == 0:
        node.get_logger().error("æœªæ‰¾åˆ°RealSenseè®¾å¤‡ï¼")
        return
    
    dev = devices[0]
    node.get_logger().info(f"âœ“ æ£€æµ‹åˆ°è®¾å¤‡: {dev.get_info(rs.camera_info.name)}")
    
    # æŸ¥è¯¢æ·±åº¦å’Œå½©è‰²ä¼ æ„Ÿå™¨æ”¯æŒçš„åˆ†è¾¨ç‡
    depth_sensor = dev.first_depth_sensor()
    color_sensor = None
    for sensor in dev.query_sensors():
        if sensor.is_color_sensor():
            color_sensor = sensor
            break
    
    # è·å–æ·±åº¦æµæ”¯æŒçš„åˆ†è¾¨ç‡
    depth_profiles = depth_sensor.get_stream_profiles()
    depth_resolutions = set()
    for profile in depth_profiles:
        if profile.stream_type() == rs.stream.depth:
            video_profile = profile.as_video_stream_profile()
            fps = video_profile.fps()
            if fps == 30:  # åªè€ƒè™‘30fps
                depth_resolutions.add((video_profile.width(), video_profile.height()))
    
    # è·å–å½©è‰²æµæ”¯æŒçš„åˆ†è¾¨ç‡
    color_resolutions = set()
    if color_sensor:
        color_profiles = color_sensor.get_stream_profiles()
        for profile in color_profiles:
            if profile.stream_type() == rs.stream.color:
                video_profile = profile.as_video_stream_profile()
                fps = video_profile.fps()
                if fps == 30 and profile.format() == rs.format.bgr8:
                    color_resolutions.add((video_profile.width(), video_profile.height()))
    
    # æ‰¾åˆ°æ·±åº¦å’Œå½©è‰²éƒ½æ”¯æŒçš„åˆ†è¾¨ç‡ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    preferred_resolutions = [(848, 480), (640, 480), (1280, 720)]
    selected_width, selected_height = 640, 480  # é»˜è®¤å€¼
    
    for pref_w, pref_h in preferred_resolutions:
        if (pref_w, pref_h) in depth_resolutions and (pref_w, pref_h) in color_resolutions:
            selected_width, selected_height = pref_w, pref_h
            node.get_logger().info(f"âœ“ é€‰æ‹©åˆ†è¾¨ç‡: {selected_width}x{selected_height} (æ·±åº¦+å½©è‰²éƒ½æ”¯æŒ)")
            break
    
    # é…ç½®ç›¸æœºæµ
    config.enable_stream(rs.stream.depth, selected_width, selected_height, rs.format.z16, 30)
    config.enable_stream(rs.stream.color, selected_width, selected_height, rs.format.bgr8, 30)
    profile = pipeline.start(config)
    align = rs.align(rs.stream.color)
    node.get_logger().info("âœ“ RealSenseç›¸æœºå¯åŠ¨æˆåŠŸï¼ˆç›´æ¥è¯»å–æ¨¡å¼ï¼‰")

    # AprilTagæ£€æµ‹å™¨å®ä¾‹
    # âš ï¸ å…³é”®å‚æ•°ï¼štag_sizeå¿…é¡»ä¸å®é™…AprilTagå°ºå¯¸ä¸€è‡´ï¼
    APRILTAG_SIZE = 0.025  # å•ä½ï¼šç±³ï¼ˆ25mmï¼‰âš ï¸ è¯·ç¡®è®¤å®é™…å°ºå¯¸ï¼
    apriltag_detector = Detector(
        families='tag25h9',
        nthreads=4,
        quad_decimate=1.0,
        quad_sigma=0.0,
        refine_edges=1,
        decode_sharpening=0.4,
        debug=0
    )
    node.get_logger().info("âœ“ AprilTagæ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
    node.get_logger().warn(f"âš ï¸  AprilTagå°ºå¯¸è®¾ç½®: {APRILTAG_SIZE*1000:.0f}mm - è¯·ç¡®è®¤å®é™…å°ºå¯¸æ˜¯å¦ä¸€è‡´ï¼")
    
    # ç›¸æœºå†…å‚ï¼ˆä»RealSenseè·å–ï¼‰
    depth_sensor = profile.get_device().first_depth_sensor()
    depth_scale = depth_sensor.get_depth_scale()
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šAprilTagæ£€æµ‹ç”¨å½©è‰²å›¾ï¼Œåº”ä½¿ç”¨å½©è‰²æµçš„å†…å‚ï¼
    color_intrin_obj = profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()
    depth_intrin_obj = profile.get_stream(rs.stream.depth).as_video_stream_profile().get_intrinsics()
    
    # AprilTagæ£€æµ‹ä½¿ç”¨å½©è‰²ç›¸æœºå†…å‚
    camera_params = [color_intrin_obj.fx, color_intrin_obj.fy, color_intrin_obj.ppx, color_intrin_obj.ppy]
    
    node.get_logger().info(f"âœ“ ç›¸æœºå†…å‚å·²åŠ è½½")
    node.get_logger().info(f"  å½©è‰²ç›¸æœº: fx={color_intrin_obj.fx:.1f}, fy={color_intrin_obj.fy:.1f}")
    node.get_logger().info(f"  æ·±åº¦ç›¸æœº: fx={depth_intrin_obj.fx:.1f}, fy={depth_intrin_obj.fy:.1f}")
    
    # æ‰‹çœ¼æ ‡å®šå‚æ•°ï¼ˆä»piper_arm.pyåŠ è½½ï¼‰
    if node.piper_arm is not None:
        from utils.utils_math import quaternion_to_rotation_matrix
        # æ„å»ºlink6åˆ°cameraçš„å˜æ¢çŸ©é˜µ
        link6_T_camera = np.eye(4)
        link6_T_camera[:3, :3] = quaternion_to_rotation_matrix(node.piper_arm.link6_q_camera)
        link6_T_camera[:3, 3] = node.piper_arm.link6_t_camera
        
        # æ‰“å°æ‰‹çœ¼æ ‡å®šå‚æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        node.get_logger().info(f"âœ“ æ‰‹çœ¼æ ‡å®šçŸ©é˜µå·²åŠ è½½")
        node.get_logger().info(f"  link6_t_camera (å¹³ç§»): {node.piper_arm.link6_t_camera}")
        node.get_logger().info(f"  link6_q_camera (å››å…ƒæ•°): {node.piper_arm.link6_q_camera}")
    else:
        link6_T_camera = np.eye(4)
        link6_T_camera[2, 3] = 0.05
        node.get_logger().warn(f"âš ï¸  ä½¿ç”¨é»˜è®¤æ‰‹çœ¼æ ‡å®šå‚æ•°")
    
    # åˆ›å»º OpenCV çª—å£
    cv2.namedWindow('detection', cv2.WINDOW_AUTOSIZE)
    cv2.setMouseCallback('detection', mouse_callback, node)

    global all_detections, selected_button_index, selected_button_locked
    global current_depth_data, current_color_data, current_depth_intrin
    global frame_counter, last_fps_time, fps_counter, current_fps
    global is_paused, paused_frame, paused_detections
    global selected_box_signature

    import signal
    import sys
    
    # æ³¨å†ŒCtrl+Cä¿¡å·å¤„ç†å™¨
    def signal_handler(sig, frame):
        print("\n\næ”¶åˆ°é€€å‡ºä¿¡å· (Ctrl+C)ï¼Œæ­£åœ¨é€€å‡º...")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        node.get_logger().info("âœ“ å¼€å§‹æ£€æµ‹...")
        node.get_logger().info("  æç¤º: æŒ‰ Ctrl+C æˆ– 'q' é”®é€€å‡º")
        
        while rclpy.ok():
            # FPSç»Ÿè®¡
            fps_counter += 1
            current_time = time.time()
            if current_time - last_fps_time >= 1.0:
                current_fps = fps_counter / (current_time - last_fps_time)
                fps_counter = 0
                last_fps_time = current_time
            
            # è·å–ç›¸æœºå¸§ï¼ˆç›´æ¥è¯»å–ï¼Œæ— å»¶è¿Ÿï¼‰
            frames = pipeline.wait_for_frames()
            aligned_frames = align.process(frames)
            
            aligned_depth = aligned_frames.get_depth_frame()
            color_frame = aligned_frames.get_color_frame()
            
            if not (aligned_depth and color_frame):
                continue
            
            depth_data = np.asanyarray(aligned_depth.get_data())
            color_data = np.asanyarray(color_frame.get_data())
            depth_intrin = aligned_depth.profile.as_video_stream_profile().intrinsics
            
            current_depth_data = depth_data
            current_color_data = color_data
            current_depth_intrin = depth_intrin
            
            # æš‚åœæ¨¡å¼
            if is_paused:
                if paused_frame is not None:
                    display_img = visualize_detections(paused_frame, paused_detections, selected_button_index, None)
                    cv2.putText(display_img, "PAUSED - Click to Select", (10, 100), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
                    cv2.imshow('detection', display_img)
                key = cv2.waitKey(1) & 0xFF
                if key == 32:  # SPACE
                    is_paused = False
                    node.get_logger().info("â–¶ï¸  ç”»é¢ç»§ç»­")
                    paused_frame = None
                    paused_detections = []
                elif key == 27:  # ESC
                    selected_button_index = -1
                    selected_button_locked = False
                    selected_box_signature = None
                elif key == ord('q'):
                    break
                continue

            # YOLOåŒæ­¥æ£€æµ‹
            color_data_small = cv2.resize(color_data, None, fx=YOLO_SCALE_FACTOR, fy=YOLO_SCALE_FACTOR, interpolation=cv2.INTER_NEAREST)
            target_boxes_small = YOLODetection(node.model, color_data_small, conf_threshold=YOLO_CONF_THRESHOLD)
            all_detections = []
            for x1, y1, x2, y2, class_name, conf in target_boxes_small:
                all_detections.append((
                    int(x1 / YOLO_SCALE_FACTOR), int(y1 / YOLO_SCALE_FACTOR),
                    int(x2 / YOLO_SCALE_FACTOR), int(y2 / YOLO_SCALE_FACTOR),
                    class_name, conf, None
                ))

            # AprilTagæ£€æµ‹ä¸å¤šåæ ‡ç³»è½¬æ¢
            gray = cv2.cvtColor(color_data, cv2.COLOR_BGR2GRAY)
            apriltag_detections = apriltag_detector.detect(
                gray,
                estimate_tag_pose=True,
                camera_params=camera_params,
                tag_size=APRILTAG_SIZE  # ä½¿ç”¨å…¨å±€è®¾ç½®çš„æ ‡ç­¾å°ºå¯¸
            )
            
            # ç»˜åˆ¶AprilTagæ£€æµ‹ç»“æœï¼ˆè¾¹æ¡†å’Œåæ ‡è½´ï¼‰
            for det in apriltag_detections:
                # ç»˜åˆ¶è¾¹æ¡†
                corners = det.corners.astype(int)
                for i in range(4):
                    pt1 = tuple(corners[i])
                    pt2 = tuple(corners[(i + 1) % 4])
                    cv2.line(color_data, pt1, pt2, (0, 255, 0), 2)
                
                # ç»˜åˆ¶ä¸­å¿ƒç‚¹
                center = det.center.astype(int)
                cv2.circle(color_data, tuple(center), 5, (0, 0, 255), -1)
                
                # ç»˜åˆ¶ID
                cv2.putText(color_data, f"ID: {det.tag_id}", 
                            (center[0] - 20, center[1] - 20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                # ç»˜åˆ¶3Dåæ ‡è½´
                if det.pose_t is not None and det.pose_R is not None:
                    # æ„å»ºåæ ‡è½´ç«¯ç‚¹ï¼ˆ3Dï¼‰
                    axis_length = 0.025  # 25mmåæ ‡è½´é•¿åº¦
                    axis_3d = np.float32([
                        [axis_length, 0, 0],  # Xè½´ - çº¢è‰²
                        [0, axis_length, 0],  # Yè½´ - ç»¿è‰²
                        [0, 0, axis_length],  # Zè½´ - è“è‰²
                        [0, 0, 0]             # åŸç‚¹
                    ]).reshape(-1, 3)
                    
                    # æŠ•å½±åˆ°å›¾åƒå¹³é¢
                    rvec, _ = cv2.Rodrigues(det.pose_R)
                    tvec = det.pose_t
                    
                    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨å½©è‰²ç›¸æœºå†…å‚çŸ©é˜µï¼ˆAprilTagæ£€æµ‹åœ¨å½©è‰²å›¾ä¸Šï¼‰
                    camera_matrix = np.array([
                        [color_intrin_obj.fx, 0, color_intrin_obj.ppx],
                        [0, color_intrin_obj.fy, color_intrin_obj.ppy],
                        [0, 0, 1]
                    ], dtype=np.float64)
                    
                    dist_coeffs = np.zeros(5, dtype=np.float64)
                    
                    imgpts, _ = cv2.projectPoints(axis_3d, rvec, tvec, camera_matrix, dist_coeffs)
                    imgpts = imgpts.astype(int)
                    
                    origin = tuple(imgpts[3].ravel())
                    # Xè½´ - çº¢è‰²
                    cv2.line(color_data, origin, tuple(imgpts[0].ravel()), (0, 0, 255), 3)
                    # Yè½´ - ç»¿è‰²
                    cv2.line(color_data, origin, tuple(imgpts[1].ravel()), (0, 255, 0), 3)
                    # Zè½´ - è“è‰²
                    cv2.line(color_data, origin, tuple(imgpts[2].ravel()), (255, 0, 0), 3)
            
            # å‡†å¤‡AprilTagä½å§¿ä¿¡æ¯ç”¨äºUIæ˜¾ç¤º
            apriltag_pose_info = []
            
            # è·å–å½“å‰å…³èŠ‚è§’åº¦ï¼ˆç”¨äºåŸºåº§åæ ‡ç³»è½¬æ¢ï¼‰
            current_joints = None
            if node.piper is not None:
                try:
                    msg = node.piper.GetArmJointMsgs()
                    current_joints = [
                        msg.joint_state.joint_1 * 1e-3 * PI / 180.0,
                        msg.joint_state.joint_2 * 1e-3 * PI / 180.0,
                        msg.joint_state.joint_3 * 1e-3 * PI / 180.0,
                        msg.joint_state.joint_4 * 1e-3 * PI / 180.0,
                        msg.joint_state.joint_5 * 1e-3 * PI / 180.0,
                        msg.joint_state.joint_6 * 1e-3 * PI / 180.0,
                    ]
                except:
                    pass
            
            for det in apriltag_detections:
                if det.pose_t is not None and det.pose_R is not None:
                    # æ„å»ºç›¸æœºåˆ°tagçš„å˜æ¢çŸ©é˜µ
                    camera_T_tag = np.eye(4)
                    camera_T_tag[:3, :3] = det.pose_R
                    camera_T_tag[:3, 3] = det.pose_t.flatten()
                    
                    # ========== ç›¸æœºåæ ‡ç³» ==========
                    x_cam, y_cam, z_cam = det.pose_t.flatten()
                    R_cam = det.pose_R
                    
                    # ğŸ” è°ƒè¯•ï¼šæ‰“å°AprilTagåŸå§‹æ£€æµ‹æ•°æ®ï¼ˆæ¯30å¸§ä¸€æ¬¡ï¼‰
                    if frame_counter % 30 == 0:
                        node.get_logger().info(f"[AprilTagåŸå§‹æ•°æ®] ID:{det.tag_id}")
                        node.get_logger().info(f"  pose_t (å¹³ç§»): [{x_cam:.4f}, {y_cam:.4f}, {z_cam:.4f}] m")
                        node.get_logger().info(f"  pose_R (æ—‹è½¬):\n{R_cam}")
                    
                    # æ¬§æ‹‰è§’è½¬æ¢å‡½æ•°
                    def rotation_matrix_to_euler(R):
                        sy = np.sqrt(R[0,0]**2 + R[1,0]**2)
                        singular = sy < 1e-6
                        if not singular:
                            roll = np.arctan2(R[2,1], R[2,2])
                            pitch = np.arctan2(-R[2,0], sy)
                            yaw = np.arctan2(R[1,0], R[0,0])
                        else:
                            roll = np.arctan2(-R[1,2], R[1,1])
                            pitch = np.arctan2(-R[2,0], sy)
                            yaw = 0
                        return math.degrees(roll), math.degrees(pitch), math.degrees(yaw)
                    
                    roll_cam, pitch_cam, yaw_cam = rotation_matrix_to_euler(R_cam)
                    
                    # ========== å¤¹çˆªåæ ‡ç³» ==========
                    gripper_T_tag = link6_T_camera @ camera_T_tag
                    x_grip, y_grip, z_grip = gripper_T_tag[:3, 3]
                    R_grip = gripper_T_tag[:3, :3]
                    roll_grip, pitch_grip, yaw_grip = rotation_matrix_to_euler(R_grip)
                    
                    # å‘å¸ƒå¤¹çˆªåæ ‡ç³»æ¬§æ‹‰è§’ï¼ˆå…¼å®¹ï¼‰
                    euler_msg = Vector3()
                    euler_msg.x = roll_grip
                    euler_msg.y = pitch_grip
                    euler_msg.z = yaw_grip
                    node.normal_pub.publish(euler_msg)
                    
                    # ========== åŸºåº§åæ ‡ç³» ==========
                    base_info = None
                    gripper_in_base_rpy = None
                    if node.piper_arm is not None and current_joints is not None:
                        try:
                            # è®¡ç®—æ­£å‘è¿åŠ¨å­¦
                            base_T_link6 = node.piper_arm.forward_kinematics(current_joints)
                            base_T_tag = base_T_link6 @ link6_T_camera @ camera_T_tag
                            
                            x_base, y_base, z_base = base_T_tag[:3, 3]
                            R_base = base_T_tag[:3, :3]
                            roll_base, pitch_base, yaw_base = rotation_matrix_to_euler(R_base)
                            
                            # å‘å¸ƒåŸºåº§åæ ‡ç³»æ¬§æ‹‰è§’ï¼ˆæ¨èä½¿ç”¨ï¼‰
                            euler_base_msg = Vector3()
                            euler_base_msg.x = roll_base
                            euler_base_msg.y = pitch_base
                            euler_base_msg.z = yaw_base
                            node.normal_base_pub.publish(euler_base_msg)
                            
                            base_info = {
                                'position': [x_base, y_base, z_base],
                                'rpy': [roll_base, pitch_base, yaw_base]
                            }
                            
                            # è®¡ç®—å¤¹çˆªåœ¨åŸºåº§ç³»ä¸‹çš„RPYï¼ˆç›´æ¥ä»FKç»“æœæå–ï¼‰
                            R_gripper_in_base = base_T_link6[:3, :3]
                            roll_grip_base, pitch_grip_base, yaw_grip_base = rotation_matrix_to_euler(R_gripper_in_base)
                            gripper_in_base_rpy = [roll_grip_base, pitch_grip_base, yaw_grip_base]
                        except:
                            pass
                    
                    # æ”¶é›†ä½å§¿ä¿¡æ¯ç”¨äºUIæ˜¾ç¤º
                    pose_info = {
                        'tag_id': det.tag_id,
                        'camera': {
                            'position': [x_cam, y_cam, z_cam],
                            'rpy': [roll_cam, pitch_cam, yaw_cam]
                        },
                        'gripper': {
                            'position': [x_grip, y_grip, z_grip],
                            'rpy': [roll_grip, pitch_grip, yaw_grip]
                        },
                        'gripper_in_base_rpy': gripper_in_base_rpy,
                        'base': base_info
                    }
                    apriltag_pose_info.append(pose_info)
                    
                    # æ¯ç§’æ‰“å°ä¸€æ¬¡åˆ°ç»ˆç«¯ï¼ˆå‡å°‘ä¿¡æ¯é‡ï¼‰
                    if frame_counter % 30 == 0:
                        node.get_logger().info(f"[AprilTag ID:{det.tag_id}] Base XYZ: ({x_base:.3f}, {y_base:.3f}, {z_base:.3f}) m")
            
            # æé†’ï¼šå¦‚æœæœªæ£€æµ‹åˆ°AprilTag
            if len(apriltag_detections) == 0 and frame_counter % 90 == 0:  # æ¯3ç§’æé†’ä¸€æ¬¡
                node.get_logger().warn("âš ï¸  æœªæ£€æµ‹åˆ°AprilTagï¼Œè¯·ç¡®ä¿æ ‡ç­¾åœ¨è§†é‡å†…ä¸”å…‰ç…§å……è¶³")
            
            # å¯è§†åŒ–ï¼ˆåŒ…å«AprilTagä½å§¿ä¿¡æ¯ï¼‰
            display_img = visualize_detections(color_data, all_detections, selected_button_index, apriltag_pose_info)
            cv2.imshow('detection', display_img)
            
            # å¸§è®¡æ•°å™¨å¢åŠ 
            frame_counter += 1
            
            # é”®ç›˜å¤„ç†
            wait_time = max(1, int(1000 / UI_REFRESH_RATE))
            key = cv2.waitKey(wait_time) & 0xFF
            
            if key == 32:  # SPACE
                is_paused = not is_paused
                if is_paused:
                    node.get_logger().info("â¸ï¸  ç”»é¢å·²æš‚åœ")
                    paused_frame = color_data.copy()
                    paused_detections = list(all_detections)
            elif key == 27:  # ESC
                node.get_logger().info("âœ— å·²å–æ¶ˆé€‰æ‹©ï¼Œè§£é™¤é”å®š")
                selected_button_index = -1
                selected_button_locked = False
                selected_box_signature = None
            elif key == 13:  # ENTER
                if selected_button_index >= 0 and selected_button_index < len(all_detections):
                    det = all_detections[selected_button_index]
                    x1, y1, x2, y2, class_name, conf, center_3d = det
                    if center_3d is None:
                        # ç®€åŒ–çš„3Dç‚¹æå–
                        cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)
                        depth = depth_data[cy, cx] * 0.001  # mm -> m
                        if depth > 0:
                            center_3d = np.array([
                                (cx - depth_intrin.ppx) * depth / depth_intrin.fx,
                                (cy - depth_intrin.ppy) * depth / depth_intrin.fy,
                                depth
                            ])
                    if center_3d is not None:
                        # å‘å¸ƒåˆ°ROS2è¯é¢˜ï¼ˆç›¸æœºåæ ‡ç³»ï¼Œå…¼å®¹ï¼‰
                        node.publish_result(center_3d, class_name)
                        
                        # è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
                        base_3d = None
                        if node.piper_arm is not None:
                            base_3d = transform_button_camera_to_base(center_3d, node.piper, node.piper_arm)
                        
                        # å‘å¸ƒåŸºåº§åæ ‡ç³»ä½ç½®
                        if base_3d is not None:
                            point_base_msg = PointStamped()
                            point_base_msg.header.stamp = node.get_clock().now().to_msg()
                            point_base_msg.header.frame_id = "arm_base"
                            point_base_msg.point.x = base_3d[0]
                            point_base_msg.point.y = base_3d[1]
                            point_base_msg.point.z = base_3d[2]
                            node.point_base_pub.publish(point_base_msg)
                        
                        # æ‰“å°è¯¦ç»†ä¿¡æ¯
                        node.get_logger().info(f"\n{'='*70}")
                        node.get_logger().info(f"âœ“ å·²ç¡®è®¤å¹¶å‘å¸ƒæŒ‰é’®:")
                        node.get_logger().info(f"  ç±»å‹: {class_name}")
                        node.get_logger().info(f"  [ç›¸æœºåæ ‡ç³»] XYZ: ({center_3d[0]:.3f}, {center_3d[1]:.3f}, {center_3d[2]:.3f}) m")
                        
                        if base_3d is not None:
                            node.get_logger().info(f"  [åŸºåº§åæ ‡ç³»] XYZ: ({base_3d[0]:.3f}, {base_3d[1]:.3f}, {base_3d[2]:.3f}) m")
                        else:
                            node.get_logger().info(f"  [åŸºåº§åæ ‡ç³»] è½¬æ¢å¤±è´¥")
                        
                        node.get_logger().info(f"{'='*70}")
                        
                        selected_button_index = -1
                        selected_button_locked = False
                        selected_box_signature = None
            
            elif key == ord('q'):
                break
            
            # ROS2 spin
            rclpy.spin_once(node, timeout_sec=0.001)
    
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
    except Exception as e:
        print(f"\nç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
    finally:
        print("æ­£åœ¨å…³é—­ç›¸æœº...")
        try:
            pipeline.stop()
        except:
            pass
        
        print("æ­£åœ¨å…³é—­çª—å£...")
        try:
            cv2.destroyAllWindows()
            cv2.waitKey(1)  # å¼ºåˆ¶åˆ·æ–°çª—å£äº‹ä»¶
        except:
            pass
        
        print("æ­£åœ¨æ¸…ç†ROS2èŠ‚ç‚¹...")
        try:
            node.destroy_node()
        except:
            pass
        
        try:
            rclpy.shutdown()
        except:
            pass
        
        print("âœ“ æ¸…ç†å®Œæˆï¼Œç¨‹åºé€€å‡º")


if __name__ == '__main__':
    main()
