#!/usr/bin/env python3
"""
äº¤äº’å¼æŒ‰é’®æ£€æµ‹å™¨ - ROS2 ç‰ˆæœ¬ï¼ˆè®¢é˜…è¯é¢˜ï¼‰
è®¢é˜… RealSense ROS2 èŠ‚ç‚¹çš„å›¾åƒè¯é¢˜
ä½¿ç”¨ yolo_button.pt æ¨¡å‹
æ”¯æŒç”¨æˆ·ç‚¹å‡»é€‰æ‹©è¦æ“ä½œçš„æŒ‰é’®

æ€§èƒ½ä¼˜åŒ–ï¼š
- é™ä½YOLOæ£€æµ‹é¢‘ç‡ï¼ˆDETECTION_SKIP_FRAMESï¼‰
- æ»‘åŠ¨çª—å£å¹³æ»‘æ£€æµ‹æ¡†ï¼ˆCACHE_SIZEï¼‰
- å‡å°‘é‡å¤æ¸²æŸ“
"""
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import cv2
from ultralytics import YOLO
from geometry_msgs.msg import PointStamped
from std_msgs.msg import String
from visualization_msgs.msg import Marker
import numpy as np
import message_filters
import math

# å¯¼å…¥åæ ‡è½¬æ¢æ‰€éœ€çš„æ¨¡å—
from piper_sdk import C_PiperInterface_V2
from piper_arm import PiperArm
from utils.utils_math import quaternion_to_rotation_matrix

PI = math.pi

# ========================================
# æ€§èƒ½è°ƒä¼˜å‚æ•°
# ========================================
DETECTION_SKIP_FRAMES = 1  # æ¯2å¸§æ£€æµ‹1æ¬¡ï¼ˆå¹³è¡¡æµç•…åº¦å’Œæ€§èƒ½ï¼‰
CACHE_SIZE = 1  # ä¸å¹³æ»‘ï¼Œç›´æ¥ä½¿ç”¨å½“å‰å¸§ç»“æœ
YOLO_CONF_THRESHOLD = 0.5  # ç½®ä¿¡åº¦é˜ˆå€¼
KEYBOARD_TIMER_HZ = 30  # é”®ç›˜å“åº”é€Ÿåº¦

# ========================================
# å…¨å±€å˜é‡
# ========================================
# æ£€æµ‹ç»“æœå­˜å‚¨
all_detections = []  # [(x1, y1, x2, y2, class_name, conf, center_3d), ...]
selected_button_index = -1  # ç”¨æˆ·é€‰ä¸­çš„æŒ‰é’®ç´¢å¼•
confirmed = False
selected_button_locked = False
selected_box_signature = None  # è®°å½•å½“å‰é€‰ä¸­çš„æ£€æµ‹æ¡†ç‰¹å¾ï¼ˆç”¨äºé‡æ–°åŒ¹é…ï¼‰

# æ£€æµ‹ç»“æœç¨³å®šæ€§æ§åˆ¶
detection_cache = []  # ç¼“å­˜æœ€è¿‘Nå¸§çš„æ£€æµ‹ç»“æœ
frame_counter = 0  # å¸§è®¡æ•°å™¨

# é¼ æ ‡ä½ç½®
mouse_x, mouse_y = 0, 0

# å½“å‰å¸§æ•°æ®
current_depth_data = None
current_color_data = None
current_camera_info = None
current_annotated_img = None  # ç¼“å­˜æ ‡æ³¨å›¾åƒï¼Œé¿å…é‡å¤ç”Ÿæˆ

# äº¤äº’æ§åˆ¶
is_paused = False  # æ˜¯å¦æš‚åœç”»é¢ï¼ˆæ–¹ä¾¿é€‰æ‹©ï¼‰
paused_frame = None  # æš‚åœæ—¶çš„å›¾åƒ
paused_detections = []  # æš‚åœæ—¶çš„æ£€æµ‹ç»“æœ

# FPSç»Ÿè®¡
import time
last_fps_time = time.time()
fps_counter = 0
current_fps = 0.0


# ========================================
# é¼ æ ‡å›è°ƒå‡½æ•°
# ========================================
def mouse_callback(event, x, y, flags, param):
    """å¤„ç†é¼ æ ‡äº‹ä»¶ï¼Œå…è®¸ç”¨æˆ·ç‚¹å‡»é€‰æ‹©æŒ‰é’®"""
    global selected_button_index, mouse_x, mouse_y, all_detections
    global current_depth_data, current_color_data, current_camera_info
    global selected_button_locked
    global is_paused, paused_detections
    global selected_box_signature
    
    mouse_x, mouse_y = x, y
    
    if event == cv2.EVENT_LBUTTONDOWN:  # å·¦é”®ç‚¹å‡»
        # ä¿®æ­£åæ ‡ï¼šæ˜¾ç¤ºå›¾åƒé¡¶éƒ¨æœ‰60åƒç´ çš„æç¤ºæ 
        TIP_BAR_HEIGHT = 60
        corrected_y = y - TIP_BAR_HEIGHT
        
        print(f"\n[é¼ æ ‡ç‚¹å‡»] åŸå§‹: ({x}, {y}), ä¿®æ­£: ({x}, {corrected_y})")
        
        # å¦‚æœç‚¹å‡»åœ¨æç¤ºæ ä¸Šï¼Œå¿½ç•¥
        if corrected_y < 0:
            return
        
        # æ£€æŸ¥ç‚¹å‡»ä½ç½®æ˜¯å¦åœ¨æŸä¸ªæ£€æµ‹æ¡†å†…
        # å¦‚æœæš‚åœï¼Œä½¿ç”¨æš‚åœæ—¶çš„æ£€æµ‹ç»“æœ
        detections_to_check = paused_detections if is_paused else all_detections
        
        found = False
        for idx, det in enumerate(detections_to_check):
            x1, y1, x2, y2, class_name, conf, center_3d = det
            
            if x1 <= x <= x2 and y1 <= corrected_y <= y2:
                print(f" â†’ âœ“ åŒ¹é…æŒ‰é’® #{idx}")
                found = True
                selected_button_index = idx
                selected_button_locked = True
                
                # ç«‹å³è®¡ç®—3Dä½ç½®
                current_det = det
                if center_3d is None and current_depth_data is not None:
                    center_3d, stats = extract_roi_cloud(
                        current_depth_data, 
                        current_color_data, 
                        [x1, y1, x2, y2], 
                        current_camera_info,
                        verbose=False,  # ç‚¹å‡»æ—¶ä¸æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œé¿å…å¡é¡¿
                        show_depth_map=False  # å¯è®¾ä¸ºTrueæŸ¥çœ‹æ·±åº¦çƒ­åŠ›å›¾
                    )
                    
                    # æ›´æ–°æ£€æµ‹ç»“æœ
                    if is_paused:
                        paused_detections[idx] = (x1, y1, x2, y2, class_name, conf, center_3d)
                        # åŒæ­¥æ›´æ–°å…¨å±€ç»“æœï¼ˆå¦‚æœå¯¹åº”çš„è¯ï¼‰
                        if idx < len(all_detections):
                            all_detections[idx] = (x1, y1, x2, y2, class_name, conf, center_3d)
                            current_det = all_detections[idx]
                        else:
                            current_det = paused_detections[idx]
                    else:
                        all_detections[idx] = (x1, y1, x2, y2, class_name, conf, center_3d)
                        current_det = all_detections[idx]

                # è®°å½•å½“å‰é€‰ä¸­æ¡†çš„ç­¾åï¼Œä¾¿äºåç»­ä¿æŒé«˜äº®
                remember_selected_detection(current_det)
                
                print(f"\n{'='*60}")
                print(f"âœ“ å·²é€‰æ‹©æŒ‰é’® #{idx}")
                print(f"  ç±»å‹: {class_name}")
                print(f"  ç½®ä¿¡åº¦: {conf:.2f}")
                if center_3d is not None:
                    print(f"  ç›¸æœºåæ ‡: ({center_3d[0]:.3f}, {center_3d[1]:.3f}, {center_3d[2]:.3f}) m")
                    # å°è¯•è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
                    if param is not None and hasattr(param, 'piper') and param.piper is not None:
                        base_3d = transform_camera_to_base(center_3d, param.piper, param.piper_arm)
                        if base_3d is not None:
                            print(f"  åŸºåº§åæ ‡: ({base_3d[0]:.3f}, {base_3d[1]:.3f}, {base_3d[2]:.3f}) m")
                print(f"{'='*60}")
                break
        
        if not found:
            print(" â†’ âœ— æœªç‚¹å‡»åˆ°æŒ‰é’®")


# ========================================
# YOLO æ£€æµ‹ï¼ˆå¸¦å¹³æ»‘ï¼‰
# ========================================
def YOLODetection(model, color_img, conf_threshold=0.5):
    """YOLO æ£€æµ‹ - å›ºå®šç¼–å·é¡ºåº"""
    results = model(color_img, verbose=False)
    target_boxes = []
    
    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            conf = float(box.conf[0])
            cls_id = int(box.cls[0])
            class_name = model.names[cls_id]
            
            if conf >= conf_threshold:
                target_boxes.append((x1, y1, x2, y2, class_name, conf))
    
    # æŒ‰ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹æ’åºï¼ˆå›ºå®šç¼–å·ï¼‰
    target_boxes.sort(key=lambda box: (box[1] // 100, box[0]))  # å…ˆæŒ‰yåæ ‡åˆ†ç»„ï¼Œå†æŒ‰xæ’åº
    
    return target_boxes


def smooth_detections(new_detections):
    """
    å¹³æ»‘æ£€æµ‹ç»“æœï¼Œå‡å°‘æŠ–åŠ¨
    ä½¿ç”¨æ»‘åŠ¨çª—å£å¹³å‡æ£€æµ‹æ¡†ä½ç½®
    """
    global detection_cache
    
    # æ·»åŠ åˆ°ç¼“å­˜
    detection_cache.append(new_detections)
    if len(detection_cache) > CACHE_SIZE:
        detection_cache.pop(0)
    
    # å¦‚æœç¼“å­˜ä¸è¶³ï¼Œç›´æ¥è¿”å›
    if len(detection_cache) < 2:
        return new_detections
    
    # å¯¹æ¯ä¸ªæ£€æµ‹æ¡†ï¼Œåœ¨ç¼“å­˜ä¸­æ‰¾åˆ°ç›¸ä¼¼çš„æ¡†å¹¶å¹³å‡
    smoothed = []
    for det in new_detections:
        x1, y1, x2, y2, class_name, conf = det
        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
        
        # åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾ç›¸ä¼¼ä½ç½®çš„åŒç±»æŒ‰é’®
        similar_boxes = []
        for cached_detections in detection_cache:
            for cached_det in cached_detections:
                cx2, cy2 = (cached_det[0] + cached_det[2]) / 2, (cached_det[1] + cached_det[3]) / 2
                distance = np.sqrt((cx - cx2)**2 + (cy - cy2)**2)
                
                # å¦‚æœä¸­å¿ƒè·ç¦»<50åƒç´  ä¸” ç±»å‹ç›¸åŒï¼Œè®¤ä¸ºæ˜¯åŒä¸€ä¸ªæŒ‰é’®
                if distance < 50 and cached_det[4] == class_name:
                    similar_boxes.append(cached_det)
        
        # å¹³å‡åæ ‡
        if len(similar_boxes) > 0:
            avg_x1 = int(np.mean([b[0] for b in similar_boxes]))
            avg_y1 = int(np.mean([b[1] for b in similar_boxes]))
            avg_x2 = int(np.mean([b[2] for b in similar_boxes]))
            avg_y2 = int(np.mean([b[3] for b in similar_boxes]))
            avg_conf = np.mean([b[5] for b in similar_boxes])
            smoothed.append((avg_x1, avg_y1, avg_x2, avg_y2, class_name, avg_conf))
        else:
            smoothed.append(det)
    
    return smoothed


def _make_signature_from_detection(det):
    """æ ¹æ®æ£€æµ‹æ¡†ç”Ÿæˆå”¯ä¸€ç­¾åï¼ˆç±»åˆ« + ä¸­å¿ƒç‚¹ï¼‰"""
    x1, y1, x2, y2, class_name, *_ = det
    center = ((x1 + x2) / 2.0, (y1 + y2) / 2.0)
    return {
        "class": class_name,
        "center": center,
        "bbox": (x1, y1, x2, y2),
    }


def remember_selected_detection(det):
    """è®°å½•å½“å‰é€‰ä¸­çš„æ£€æµ‹æ¡†ç‰¹å¾"""
    global selected_box_signature
    selected_box_signature = _make_signature_from_detection(det)


def sync_selection_with_detections():
    """åœ¨æ£€æµ‹ç»“æœæ›´æ–°åï¼Œé‡æ–°å…³è”å·²é€‰ä¸­çš„æŒ‰é’®"""
    global selected_button_index, selected_box_signature

    if selected_box_signature is None or not all_detections:
        return

    best_idx = -1
    best_dist = float('inf')
    target_class = selected_box_signature["class"]
    target_center = selected_box_signature["center"]

    for idx, det in enumerate(all_detections):
        x1, y1, x2, y2, class_name, *_ = det
        if class_name != target_class:
            continue
        center = ((x1 + x2) / 2.0, (y1 + y2) / 2.0)
        dist = np.linalg.norm(np.array(center) - np.array(target_center))
        if dist < best_dist:
            best_dist = dist
            best_idx = idx

    # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿæ¥è¿‘çš„æ¡†ï¼Œåˆ™æ›´æ–°ç´¢å¼•ï¼›å¦åˆ™æ¸…ç©ºé€‰æ‹©
    if best_idx != -1 and best_dist < 80:  # 80åƒç´ é˜ˆå€¼ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
        selected_button_index = best_idx
    else:
        selected_button_index = -1
        selected_box_signature = None


# ========================================
# å¯è§†åŒ–
# ========================================
def visualize_detections(color_img, detections, selected_idx):
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
    annotated = color_img.copy()
    
    for idx, det in enumerate(detections):
        x1, y1, x2, y2, class_name, conf, center_3d = det
        
        # é€‰ä¸­çš„æŒ‰é’®ç”¨ç»¿è‰²ï¼Œå…¶ä»–ç”¨è“è‰²
        if idx == selected_idx:
            color = (0, 255, 0)  # ç»¿è‰²
            thickness = 3
        else:
            color = (255, 0, 0)  # è“è‰²
            thickness = 2
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, thickness)
        
        # æ˜¾ç¤ºæ ‡ç­¾å’Œç½®ä¿¡åº¦
        label = f"#{idx} {class_name} {conf:.2f}"
        cv2.putText(
            annotated, label, (x1, y1 - 5),
            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2
        )
        
        # æ˜¾ç¤º3Dåæ ‡
        if center_3d is not None:
            coord_text = f"({center_3d[0]:.2f}, {center_3d[1]:.2f}, {center_3d[2]:.2f})"
            cv2.putText(
                annotated, coord_text, (x1, y2 + 20),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1
            )
    
    # é¡¶éƒ¨æç¤ºï¼ˆåŒ…å«FPSï¼‰
    tip_bg = np.zeros((60, annotated.shape[1], 3), dtype=np.uint8)
    tip_bg[:] = (50, 50, 50)
    
    cv2.putText(
        tip_bg, "Step 1: Click on a button to select", (10, 20),
        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2
    )
    cv2.putText(
        tip_bg, "Step 2: Press ENTER to confirm | ESC to cancel", (10, 45),
        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2
    )
    
    # æç¤ºç©ºæ ¼é”®æš‚åœ
    cv2.putText(
        tip_bg, "[SPACE] Pause/Resume", (annotated.shape[1] - 350, 35),
        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1
    )
    
    # æ˜¾ç¤ºFPS
    global current_fps
    fps_text = f"FPS: {current_fps:.1f}"
    cv2.putText(
        tip_bg, fps_text, (annotated.shape[1] - 120, 35),
        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
    )
    
    annotated = np.vstack([tip_bg, annotated])
    
    # ä¸æ˜¾ç¤ºé¼ æ ‡ä½ç½®ï¼ˆé¿å…å¡é¡¿ï¼‰
    
    return annotated


# ========================================
# ROI ç‚¹äº‘æå–
# ========================================
def extract_roi_cloud(depth_data, color_data, bbox, camera_info, verbose=False, show_depth_map=False):
    """ä»æ·±åº¦å›¾ä¸­æå– ROI åŒºåŸŸçš„ 3D ä¸­å¿ƒç‚¹ï¼Œå¹¶è¿”å›å®Œæ•´ç‚¹äº‘ç”¨äºdebug"""
    x1, y1, x2, y2 = bbox
    
    # è®¡ç®—2Dè¾¹ç•Œæ¡†çš„ä¸­å¿ƒç‚¹
    center_u = int((x1 + x2) / 2)
    center_v = int((y1 + y2) / 2)
    
    # è·å–ç›¸æœºå†…å‚
    fx = camera_info.k[0]
    fy = camera_info.k[4]
    cx = camera_info.k[2]
    cy = camera_info.k[5]
    
    # ========== 1. å¿«é€Ÿç»Ÿè®¡ROIæ·±åº¦ä¿¡æ¯ï¼ˆä¸ç”Ÿæˆå®Œæ•´ç‚¹äº‘ï¼‰ ==========
    roi_depth = depth_data[y1:y2, x1:x2]
    valid_roi_depths = roi_depth[roi_depth > 0]
    
    # å¯é€‰ï¼šæ˜¾ç¤ºæ·±åº¦å›¾çƒ­åŠ›å›¾ï¼ˆdebugç”¨ï¼‰
    if show_depth_map and roi_depth.size > 0:
        # å½’ä¸€åŒ–æ·±åº¦å›¾ç”¨äºå¯è§†åŒ–
        depth_normalized = roi_depth.copy().astype(float)
        depth_normalized[depth_normalized == 0] = np.nan  # æ— æ•ˆå€¼è®¾ä¸ºnan
        if np.nanmax(depth_normalized) > 0:
            depth_normalized = (depth_normalized - np.nanmin(depth_normalized)) / (np.nanmax(depth_normalized) - np.nanmin(depth_normalized)) * 255
            depth_normalized = np.nan_to_num(depth_normalized, nan=0).astype(np.uint8)
            depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
            # æ”¾å¤§æ˜¾ç¤º
            depth_colormap_large = cv2.resize(depth_colormap, (200, 200), interpolation=cv2.INTER_NEAREST)
            cv2.imshow('ROI Depth Map', depth_colormap_large)

    
    # ç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯ï¼ˆè½»é‡çº§ï¼‰
    point_cloud_stats = {
        'bbox_size': (x2-x1, y2-y1),
        'total_pixels': roi_depth.size,
        'valid_count': len(valid_roi_depths),
        'coverage': 100 * len(valid_roi_depths) / roi_depth.size if roi_depth.size > 0 else 0,
        'depth_range': (valid_roi_depths.min(), valid_roi_depths.max()) if len(valid_roi_depths) > 0 else (0, 0),
        'depth_mean': valid_roi_depths.mean() if len(valid_roi_depths) > 0 else 0,
        'depth_std': valid_roi_depths.std() if len(valid_roi_depths) > 0 else 0,
    }
    
    # åªåœ¨verboseæ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†ä¿¡æ¯
    if verbose:
        print(f"\n{'='*60}")
        print(f"[ROI] å°ºå¯¸:{point_cloud_stats['bbox_size'][0]}x{point_cloud_stats['bbox_size'][1]} "
              f"è¦†ç›–:{point_cloud_stats['coverage']:.1f}% "
              f"æ·±åº¦:{point_cloud_stats['depth_mean']:.0f}Â±{point_cloud_stats['depth_std']:.0f}mm")
    
    # ========== 2. æ™ºèƒ½é‡‡æ ·ç­–ç•¥ï¼šä½¿ç”¨ä¸­ä½æ•°+ç¦»ç¾¤ç‚¹è¿‡æ»¤ ==========
    sample_size = 5  # æ‰©å¤§é‡‡æ ·çª—å£åˆ°5x5ï¼ˆæ›´å¤šæ ·æœ¬ï¼‰
    u_min = max(0, center_u - sample_size)
    u_max = min(depth_data.shape[1], center_u + sample_size)
    v_min = max(0, center_v - sample_size)
    v_max = min(depth_data.shape[0], center_v + sample_size)
    
    depth_window = depth_data[v_min:v_max, u_min:u_max]
    valid_depths = depth_window[depth_window > 0]
    
    if len(valid_depths) == 0:
        if verbose:
            print(f"[ä¸­å¿ƒç‚¹] âš ï¸ æ— æœ‰æ•ˆæ·±åº¦")
            print(f"{'='*60}\n")
        return None, point_cloud_stats
    
    # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨ä¸­ä½æ•°è€Œä¸æ˜¯å¹³å‡å€¼ï¼Œæ›´æŠ—ç¦»ç¾¤ç‚¹
    depth_median = np.median(valid_depths)
    depth_mean = np.mean(valid_depths)
    depth_std = np.std(valid_depths)
    
    # å¦‚æœæ ‡å‡†å·®å¤ªå¤§ï¼ˆ>200mmï¼‰ï¼Œè¯´æ˜æœ‰ç¦»ç¾¤ç‚¹ï¼Œè¿‡æ»¤æ‰
    if depth_std > 200:
        # ä½¿ç”¨ median Â± 1.5*std è¿‡æ»¤ç¦»ç¾¤ç‚¹
        lower_bound = depth_median - 1.5 * depth_std
        upper_bound = depth_median + 1.5 * depth_std
        filtered_depths = valid_depths[(valid_depths >= lower_bound) & (valid_depths <= upper_bound)]
        
        if len(filtered_depths) > 0:
            if verbose:
                print(f"[ä¸­å¿ƒç‚¹] âš ï¸ æ£€æµ‹åˆ°ç¦»ç¾¤ç‚¹ï¼Œè¿‡æ»¤å‰:{len(valid_depths)}ä¸ª â†’ è¿‡æ»¤å:{len(filtered_depths)}ä¸ª")
            valid_depths = filtered_depths
            depth_median = np.median(valid_depths)
    
    # ä¼˜å…ˆä½¿ç”¨ä¸­ä½æ•°ï¼ˆæ›´é²æ£’ï¼‰
    depth_value = depth_median
    depth_m = depth_value / 1000.0
    
    # æ‰©å¤§åˆç†èŒƒå›´ï¼š0.15m ~ 1.5mï¼ˆæŒ‰é’®æ“ä½œçš„å®é™…è·ç¦»ï¼‰
    if depth_m < 0.15 or depth_m > 1.5:
        if verbose:
            print(f"[ä¸­å¿ƒç‚¹] âš ï¸ æ·±åº¦å¼‚å¸¸: median={depth_median:.0f}mm ({depth_m:.3f}m), mean={depth_mean:.0f}mm, std={depth_std:.0f}mm")
            print(f"         å¯èƒ½åŸå› : æè´¨åå…‰/è¾¹ç¼˜æ•ˆåº”/æ·±åº¦å­”æ´")
            print(f"{'='*60}\n")
        return None, point_cloud_stats
    
    # ä½¿ç”¨é’ˆå­”ç›¸æœºæ¨¡å‹è®¡ç®—3Dåæ ‡
    x = (center_u - cx) * depth_m / fx
    y = (center_v - cy) * depth_m / fy
    z = depth_m
    
    if verbose:
        print(f"[ä¸­å¿ƒç‚¹] æ·±åº¦:{depth_value:.0f}mm â†’ 3D:({x:.3f}, {y:.3f}, {z:.3f})m")
        print(f"{'='*60}\n")
    
    return [x, y, z], point_cloud_stats


# ========================================
# åæ ‡è½¬æ¢ï¼šç›¸æœº â†’ åŸºåº§
# ========================================
def transform_camera_to_base(button_camera, piper, piper_arm):
    """
    å°†ç›¸æœºåæ ‡ç³»çš„æŒ‰é’®ä½ç½®è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
    
    Args:
        button_camera: [x, y, z] ç›¸æœºåæ ‡ç³»
        piper: Piper SDKå®ä¾‹
        piper_arm: PiperArmå®ä¾‹
    
    Returns:
        button_base: [x, y, z] åŸºåº§åæ ‡ç³»
    """
    try:
        # è·å–å½“å‰å…³èŠ‚è§’åº¦
        msg = piper.GetArmJointMsgs()
        current_joints = [
            msg.joint_state.joint_1 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_2 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_3 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_4 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_5 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_6 * 1e-3 * PI / 180.0,
        ]
        
        # åŸºåº§åˆ°link6çš„å˜æ¢
        base_T_link6 = piper_arm.forward_kinematics(current_joints)
        
        # link6åˆ°ç›¸æœºçš„å˜æ¢ï¼ˆæ‰‹çœ¼æ ‡å®šï¼‰
        link6_T_cam = np.eye(4)
        link6_T_cam[:3, :3] = quaternion_to_rotation_matrix(piper_arm.link6_q_camera)
        link6_T_cam[:3, 3] = piper_arm.link6_t_camera
        
        # æŒ‰é’®åœ¨ç›¸æœºåæ ‡ç³»çš„é½æ¬¡åæ ‡
        button_cam_h = np.array([button_camera[0], button_camera[1], button_camera[2], 1.0])
        
        # è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
        button_base = base_T_link6 @ link6_T_cam @ button_cam_h
        
        return button_base[:3]
    except Exception as e:
        print(f"  âš ï¸  åæ ‡è½¬æ¢å¤±è´¥: {e}")
        return None


# ========================================
# ROS2 èŠ‚ç‚¹
# ========================================
class ButtonDetectorNode(Node):
    """æŒ‰é’®æ£€æµ‹èŠ‚ç‚¹ - è®¢é˜… ROS2 è¯é¢˜ç‰ˆæœ¬"""
    
    def __init__(self):
        super().__init__('button_detector_ros2')
        
        self.get_logger().info("="*70)
        self.get_logger().info("äº¤äº’å¼æŒ‰é’®æ£€æµ‹å™¨ - ROS2 ç‰ˆæœ¬ï¼ˆè®¢é˜…è¯é¢˜ï¼‰")
        self.get_logger().info("="*70)
        self.get_logger().info(f"æ€§èƒ½é…ç½®:")
        self.get_logger().info(f"  - YOLOæ£€æµ‹é¢‘ç‡: æ¯{DETECTION_SKIP_FRAMES+1}å¸§1æ¬¡")
        self.get_logger().info(f"  - å¹³æ»‘çª—å£å¤§å°: {CACHE_SIZE}å¸§")
        self.get_logger().info(f"  - ç½®ä¿¡åº¦é˜ˆå€¼: {YOLO_CONF_THRESHOLD}")
        self.get_logger().info("="*70)
        
        # åˆå§‹åŒ– Piper SDKï¼ˆç”¨äºåæ ‡è½¬æ¢ï¼‰
        try:
            self.piper = C_PiperInterface_V2("can0")
            self.piper.ConnectPort()
            self.piper_arm = PiperArm()
            self.get_logger().info("âœ“ Piper SDK åˆå§‹åŒ–æˆåŠŸï¼ˆç”¨äºåæ ‡è½¬æ¢ï¼‰")
        except Exception as e:
            self.get_logger().warn(f"âš ï¸  Piper SDK åˆå§‹åŒ–å¤±è´¥: {e}")
            self.get_logger().warn("   å°†åªæ˜¾ç¤ºç›¸æœºåæ ‡ç³»åæ ‡")
            self.piper = None
            self.piper_arm = None
        
        # åˆå§‹åŒ– CV Bridge
        self.bridge = CvBridge()
        
        # åŠ è½½ YOLO æ¨¡å‹
        self.model = YOLO('yolo_button.pt')
        self.get_logger().info("âœ“ YOLO æ¨¡å‹åŠ è½½æˆåŠŸ: yolo_button.pt")
        
        # åˆ›å»ºå‘å¸ƒå™¨
        self.point_pub = self.create_publisher(PointStamped, '/object_point', 10)
        self.type_pub = self.create_publisher(String, '/button_type', 10)
        self.marker_pub = self.create_publisher(Marker, '/object_center_marker', 10)
        
        self.get_logger().info("âœ“ å‘å¸ƒå™¨å·²åˆ›å»º")
        self.get_logger().info("  - /object_point")
        self.get_logger().info("  - /button_type")
        self.get_logger().info("  - /object_center_marker")
        
        # è®¢é˜…ç›¸æœºè¯é¢˜ï¼ˆä½¿ç”¨æ—¶é—´åŒæ­¥ï¼‰
        self.color_sub = message_filters.Subscriber(
            self, Image, '/camera/camera/color/image_raw'
        )
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šè®¢é˜…å¯¹é½åˆ°å½©è‰²å›¾çš„æ·±åº¦å›¾ï¼Œè€Œä¸æ˜¯åŸå§‹æ·±åº¦å›¾
        self.depth_sub = message_filters.Subscriber(
            self, Image, '/camera/camera/aligned_depth_to_color/image_raw'
        )
        self.camera_info_sub = message_filters.Subscriber(
            self, CameraInfo, '/camera/camera/aligned_depth_to_color/camera_info'
        )
        
        # æ—¶é—´åŒæ­¥å™¨
        self.ts = message_filters.ApproximateTimeSynchronizer(
            [self.color_sub, self.depth_sub, self.camera_info_sub],
            queue_size=10,
            slop=0.1
        )
        self.ts.registerCallback(self.image_callback)
        
        self.get_logger().info("âœ“ å·²è®¢é˜…ç›¸æœºè¯é¢˜ï¼ˆä½¿ç”¨å¯¹é½æ·±åº¦å›¾ï¼‰")
        self.get_logger().info("  - /camera/camera/color/image_raw")
        self.get_logger().info("  - /camera/camera/aligned_depth_to_color/image_raw  ğŸ”¥ å¯¹é½ç‰ˆæœ¬")
        self.get_logger().info("  - /camera/camera/aligned_depth_to_color/camera_info")
        
        # åˆ›å»º OpenCV çª—å£
        cv2.namedWindow('detection', cv2.WINDOW_AUTOSIZE)
        # å°†selfä½œä¸ºparamä¼ é€’ç»™é¼ æ ‡å›è°ƒï¼Œä»¥ä¾¿è®¿é—®piperå®ä¾‹
        cv2.setMouseCallback('detection', mouse_callback, self)
        
        # åˆ›å»ºå®šæ—¶å™¨å¤„ç†é”®ç›˜è¾“å…¥ï¼ˆé™ä½é¢‘ç‡ï¼Œå‡å°‘CPUå ç”¨ï¼‰
        self.timer = self.create_timer(1.0 / KEYBOARD_TIMER_HZ, self.keyboard_callback)
        
        self.get_logger().info("="*70)
        self.get_logger().info("âœ“ åˆå§‹åŒ–å®Œæˆï¼Œç­‰å¾…å›¾åƒ...")
        self.get_logger().info("="*70)
    
    def image_callback(self, color_msg, depth_msg, camera_info_msg):
        """å›¾åƒè¯é¢˜å›è°ƒ - ä¼˜åŒ–ç‰ˆï¼šé™ä½æ£€æµ‹é¢‘ç‡ä½†ä¿æŒæ˜¾ç¤ºæµç•…"""
        global all_detections, selected_button_index, selected_button_locked
        global current_depth_data, current_color_data, current_camera_info
        global frame_counter, current_annotated_img
        global last_fps_time, fps_counter, current_fps
        global is_paused, paused_frame, paused_detections
        
        try:
            # FPSç»Ÿè®¡
            fps_counter += 1
            current_time = time.time()
            if current_time - last_fps_time >= 1.0:
                current_fps = fps_counter / (current_time - last_fps_time)
                fps_counter = 0
                last_fps_time = current_time
            
            # å¦‚æœå¤„äºæš‚åœçŠ¶æ€ï¼Œåªæ˜¾ç¤ºæš‚åœå¸§ï¼Œä¸æ›´æ–°æ•°æ®
            if is_paused:
                if paused_frame is not None:
                    # åœ¨æš‚åœå¸§ä¸Šç»˜åˆ¶å½“å‰çš„é€‰ä¸­çŠ¶æ€
                    display_img = visualize_detections(
                        paused_frame, paused_detections, selected_button_index
                    )
                    # æ·»åŠ æš‚åœæç¤º
                    cv2.putText(display_img, "PAUSED - Click to Select", (10, 100), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
                    cv2.imshow('detection', display_img)
                return

            # è½¬æ¢ ROS å›¾åƒæ¶ˆæ¯ä¸º OpenCV æ ¼å¼
            color_data_full = self.bridge.imgmsg_to_cv2(color_msg, desired_encoding='bgr8')
            depth_data = self.bridge.imgmsg_to_cv2(depth_msg, desired_encoding='16UC1')
            
            # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºæ·±åº¦é‡‡æ ·
            current_depth_data = depth_data
            current_color_data = color_data_full
            current_camera_info = camera_info_msg
            
            # éš”å¸§æ£€æµ‹ï¼ˆé™ä½CPUå ç”¨ï¼‰
            frame_counter += 1
            should_detect = (frame_counter % (DETECTION_SKIP_FRAMES + 1) == 0)
            
            if should_detect:
                # é™ä½åˆ†è¾¨ç‡ç”¨äºYOLOæ£€æµ‹ï¼ˆæé€Ÿ3-4å€ï¼‰
                scale_factor = 0.5  # ç¼©å°åˆ°åŸæ¥çš„ä¸€åŠ
                color_data_small = cv2.resize(color_data_full, None, fx=scale_factor, fy=scale_factor, 
                                              interpolation=cv2.INTER_LINEAR)
                
                # ä½¿ç”¨ç¼©å°çš„å›¾åƒè¿›è¡ŒYOLOæ£€æµ‹
                target_boxes_small = YOLODetection(self.model, color_data_small, conf_threshold=YOLO_CONF_THRESHOLD)
                
                # å°†æ£€æµ‹æ¡†åæ ‡ç¼©æ”¾å›åŸå§‹å°ºå¯¸
                target_boxes = []
                for x1, y1, x2, y2, class_name, conf in target_boxes_small:
                    target_boxes.append((
                        int(x1 / scale_factor),
                        int(y1 / scale_factor),
                        int(x2 / scale_factor),
                        int(y2 / scale_factor),
                        class_name,
                        conf
                    ))
                
                # ç›´æ¥æ›´æ–°å…¨å±€æ£€æµ‹ç»“æœï¼ˆä¸å¹³æ»‘ï¼Œé¿å…å»¶è¿Ÿï¼‰
                all_detections = []
                for box in target_boxes:
                    x1, y1, x2, y2, class_name, conf = box
                    all_detections.append((x1, y1, x2, y2, class_name, conf, None))

                # é‡æ–°å…³è”é€‰ä¸­çŠ¶æ€
                sync_selection_with_detections()
            
            # æ¯å¸§éƒ½é‡æ–°ç»˜åˆ¶ï¼ˆä¿æŒé€‰ä¸­æ¡†å®æ—¶æ›´æ–°ï¼‰
            display_img = visualize_detections(
                color_data_full, all_detections, selected_button_index
            )
            
            # æ˜¾ç¤ºå›¾åƒ
            cv2.imshow('detection', display_img)
            
        except Exception as e:
            self.get_logger().error(f"å¤„ç†å›¾åƒé”™è¯¯: {e}")
    
    def keyboard_callback(self):
        """å¤„ç†é”®ç›˜è¾“å…¥"""
        global all_detections, selected_button_index, selected_button_locked
        global current_depth_data, current_color_data, current_camera_info
        global is_paused, paused_frame, paused_detections
        global selected_box_signature
        
        key = cv2.waitKey(1) & 0xFF
        
        if key == 32:  # SPACE - æš‚åœ/ç»§ç»­
            is_paused = not is_paused
            if is_paused:
                self.get_logger().info("â¸ï¸  ç”»é¢å·²æš‚åœ - è¯·ç‚¹å‡»é€‰æ‹©æŒ‰é’®")
                # ä¿å­˜å½“å‰çŠ¶æ€
                if current_color_data is not None:
                    paused_frame = current_color_data.copy()
                    paused_detections = list(all_detections)
            else:
                self.get_logger().info("â–¶ï¸  ç”»é¢ç»§ç»­")
                paused_frame = None
                paused_detections = []
        
        elif key == 27:  # ESC - å–æ¶ˆé€‰æ‹©
            self.get_logger().info("âœ— å·²å–æ¶ˆé€‰æ‹©")
            selected_button_index = -1
            selected_button_locked = False
            selected_box_signature = None
        
        elif key == 13:  # ENTER - ç¡®è®¤é€‰æ‹©
            # ç¡®å®šè¦ä½¿ç”¨çš„æ£€æµ‹åˆ—è¡¨
            detections_to_use = paused_detections if is_paused else all_detections
            
            if selected_button_index >= 0 and selected_button_index < len(detections_to_use):
                det = detections_to_use[selected_button_index]
                x1, y1, x2, y2, class_name, conf, center_3d = det
                
                # è®¡ç®—3Dä½ç½®
                if center_3d is None:
                    center_3d, stats = extract_roi_cloud(
                        current_depth_data, current_color_data, 
                        [x1, y1, x2, y2], 
                        current_camera_info,
                        verbose=True,  # ç¡®è®¤æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯ç”¨äºdebug
                        show_depth_map=True  # æ˜¾ç¤ºæ·±åº¦çƒ­åŠ›å›¾å¸®åŠ©è¯Šæ–­
                    )
                    
                    # æ˜¾ç¤ºç®€æ´çš„ç»Ÿè®¡ä¿¡æ¯
                    if stats:
                        self.get_logger().info(
                            f"[ROI] å°ºå¯¸:{stats['bbox_size'][0]}x{stats['bbox_size'][1]} "
                            f"è¦†ç›–ç‡:{stats['coverage']:.1f}% "
                            f"æ·±åº¦:{stats['depth_mean']:.0f}Â±{stats['depth_std']:.0f}mm"
                        )
                    
                    # æ›´æ–°åˆ—è¡¨
                    if is_paused:
                        paused_detections[selected_button_index] = (x1, y1, x2, y2, class_name, conf, center_3d)
                    else:
                        all_detections[selected_button_index] = (x1, y1, x2, y2, class_name, conf, center_3d)
                
                if center_3d is not None:
                    self.get_logger().info(f"\n{'='*70}")
                    self.get_logger().info(f"âœ“ ç¡®è®¤é€‰æ‹©å¹¶å‘å¸ƒ:")
                    self.get_logger().info(f"  æŒ‰é’®ç±»å‹: {class_name}")
                    self.get_logger().info(f"  ç½®ä¿¡åº¦: {conf:.2f}")
                    self.get_logger().info(f"  ç›¸æœºåæ ‡: ({center_3d[0]:.3f}, {center_3d[1]:.3f}, {center_3d[2]:.3f}) m")
                    
                    # æ˜¾ç¤ºåŸºåº§åæ ‡ç³»åæ ‡
                    if self.piper is not None:
                        base_3d = transform_camera_to_base(center_3d, self.piper, self.piper_arm)
                        if base_3d is not None:
                            self.get_logger().info(f"  åŸºåº§åæ ‡: ({base_3d[0]:.3f}, {base_3d[1]:.3f}, {base_3d[2]:.3f}) m")
                    
                    self.get_logger().info(f"{'='*70}")
                    
                    # å‘å¸ƒåˆ°ROSè¯é¢˜
                    point_msg = PointStamped()
                    point_msg.header.stamp = self.get_clock().now().to_msg()
                    point_msg.header.frame_id = "camera"  # âœ“ ä¿®æ”¹ä¸ºä¸TFå‘å¸ƒå™¨ä¸€è‡´
                    point_msg.point.x = center_3d[0]
                    point_msg.point.y = center_3d[1]
                    point_msg.point.z = center_3d[2]
                    self.point_pub.publish(point_msg)
                    
                    type_msg = String()
                    type_msg.data = class_name
                    self.type_pub.publish(type_msg)
                    
                    # å‘å¸ƒå¯è§†åŒ– Marker
                    marker = Marker()
                    marker.header.frame_id = "camera"  # âœ“ ä¿®æ”¹ä¸ºä¸TFå‘å¸ƒå™¨ä¸€è‡´
                    marker.header.stamp = self.get_clock().now().to_msg()
                    marker.ns = "button_center"
                    marker.id = 0
                    marker.type = Marker.SPHERE
                    marker.action = Marker.ADD
                    marker.pose.position.x = center_3d[0]
                    marker.pose.position.y = center_3d[1]
                    marker.pose.position.z = center_3d[2]
                    marker.scale.x = 0.02
                    marker.scale.y = 0.02
                    marker.scale.z = 0.02
                    marker.color.r = 1.0
                    marker.color.g = 0.0
                    marker.color.b = 0.0
                    marker.color.a = 1.0
                    self.marker_pub.publish(marker)
                    
                    self.get_logger().info("âœ“ å·²å‘å¸ƒåˆ° ROS2 è¯é¢˜")
                    
                    # é‡ç½®é€‰æ‹©
                    selected_button_index = -1
                    selected_button_locked = False
                    selected_box_signature = None
        
        elif key == ord('q'):  # é€€å‡º
            self.get_logger().info("é€€å‡ºç¨‹åº...")
            raise KeyboardInterrupt
    
    def destroy_node(self):
        """æ¸…ç†èµ„æº"""
        cv2.destroyAllWindows()
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    
    try:
        node = ButtonDetectorNode()
        rclpy.spin(node)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if rclpy.ok():
            rclpy.shutdown()


if __name__ == '__main__':
    main()
