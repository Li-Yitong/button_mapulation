#!/usr/bin/env python3
"""
äº¤äº’å¼æŒ‰é’®æ£€æµ‹å™¨ - ROS2 æ··åˆç‰ˆæœ¬
ä½¿ç”¨ pyrealsense2 ç›´æ¥è¯»å–ç›¸æœºï¼ˆé«˜æ€§èƒ½ï¼‰+ ROS2 å‘å¸ƒç»“æœ
è§£å†³è®¢é˜…è¯é¢˜å¯¼è‡´çš„å¡é¡¿å’Œå¹»å½±é—®é¢˜
ä¿®å¤ï¼šROS2 spinçº¿ç¨‹ç‹¬ç«‹è¿è¡Œï¼Œé¿å…é˜»å¡
æ–°å¢ï¼šé¢æ¿æ³•å‘é‡è®¡ç®—ï¼ˆæ”¯æŒå€¾æ–œé¢æ¿ï¼‰
"""
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PointStamped, Vector3
from std_msgs.msg import String
from visualization_msgs.msg import Marker
import cv2
from ultralytics import YOLO
import numpy as np
import pyrealsense2 as rs
import time
import threading
from queue import Queue

# å¯¼å…¥åæ ‡è½¬æ¢æ‰€éœ€çš„æ¨¡å—
from piper_sdk import C_PiperInterface_V2
from piper_arm import PiperArm
from utils.utils_math import quaternion_to_rotation_matrix
import math

# å¯¼å…¥é¢æ¿æ³•å‘é‡è®¡ç®—å·¥å…·
from utils.utils_plane import (
    compute_robust_panel_normal, 
    compute_panel_normal_from_blue_region, 
    visualize_panel_normal,
    BLUE_HSV_LOWER as UTILS_HSV_LOWER,  # å¯¼å…¥ utils_plane.py ä¸­çš„å‚æ•°
    BLUE_HSV_UPPER as UTILS_HSV_UPPER
)

PI = math.pi

# ========================================
# æ€§èƒ½è°ƒä¼˜å‚æ•°
# ========================================
DETECTION_SKIP_FRAMES = 0  # ğŸ”§ å¼‚æ­¥æ¨¡å¼ï¼šæ¯å¸§éƒ½æ”¾å…¥é˜Ÿåˆ—ï¼Œæ£€æµ‹çº¿ç¨‹è‡ªåŠ¨å¤„ç†æœ€æ–°å¸§
YOLO_CONF_THRESHOLD = 0.4  # ğŸ”§ é™ä½é˜ˆå€¼æé«˜å¬å›ç‡ï¼ˆå°å›¾åƒéœ€è¦ï¼‰
YOLO_SCALE_FACTOR = 0.1    # ğŸ”§ ğŸš€ æé™æ¨¡å¼ï¼š640x480 â†’ 64x48 (100å€åŠ é€Ÿ!)
UI_REFRESH_RATE = 30       # ğŸ”§ UIåˆ·æ–°ç‡ï¼ˆHzï¼‰ï¼Œç‹¬ç«‹äºæ£€æµ‹é¢‘ç‡

# ========================================
# å…¨å±€å˜é‡
# ========================================
all_detections = []
selected_button_index = -1
selected_button_locked = False
selected_box_signature = None

# ğŸ”§ å¼‚æ­¥æ£€æµ‹ç›¸å…³
detection_lock = threading.Lock()  # ä¿æŠ¤æ£€æµ‹ç»“æœçš„é”
detection_queue = Queue(maxsize=5)  # ğŸ”§ å¾…æ£€æµ‹å¸§é˜Ÿåˆ—ï¼ˆç¼“å­˜5å¸§ï¼Œæ£€æµ‹çº¿ç¨‹è‡ªåŠ¨å–æœ€æ–°ï¼‰
detection_running = True  # æ£€æµ‹çº¿ç¨‹è¿è¡Œæ ‡å¿—
detection_frozen = False  # ğŸ”§ æ£€æµ‹å†»ç»“æ ‡å¿—ï¼ˆé€‰ä¸­ååœæ­¢æ£€æµ‹æ›´æ–°ï¼‰

# é¼ æ ‡ä½ç½®
mouse_x, mouse_y = 0, 0

# å½“å‰å¸§æ•°æ®
current_depth_data = None
current_color_data = None
current_depth_intrin = None

# äº¤äº’æ§åˆ¶
is_paused = False
paused_frame = None
paused_detections = []

# å¸§è®¡æ•°å™¨
frame_counter = 0

# FPSç»Ÿè®¡
last_fps_time = time.time()
fps_counter = 0
current_fps = 0.0
last_detect_time_ms = 0.0  # æœ€è¿‘ä¸€æ¬¡æ£€æµ‹è€—æ—¶

# é¢æ¿æ³•å‘é‡ç¼“å­˜
global_panel_normal = None
global_panel_normal_base = None  # ğŸ”§ å®æ—¶å­˜å‚¨åŸºåº§ç³»æ³•å‘é‡
global_panel_info = None
panel_last_update = 0.0
PANEL_CACHE_TIME = 0.0  # ï¿½ è®¾ä¸º0è¡¨ç¤ºæ¯æ¬¡æ£€æµ‹éƒ½æ›´æ–°æ³•å‘é‡ï¼Œå®ç°å®æ—¶æ•ˆæœ

# ğŸ¨ HSVé¢œè‰²è¿‡æ»¤å‚æ•°ï¼ˆè“è‰²é¢æ¿ï¼‰
# ä½¿ç”¨ tune_blue_hsv.py è°ƒè¯•å¾—åˆ°çš„æœ€ä½³å‚æ•°
# å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼šexport BLUE_HSV_LOWER="92,108,43" BLUE_HSV_UPPER="111,179,244"
import os
def _parse_hsv_env(var_name, default):
    env_val = os.environ.get(var_name)
    if env_val:
        try:
            return np.array([int(x) for x in env_val.split(',')])
        except:
            print(f"âš ï¸  ç¯å¢ƒå˜é‡ {var_name} æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
    return default

# ğŸ”§ ä½¿ç”¨ utils_plane.py ä¸­ç²¾å¿ƒè°ƒæ•´è¿‡çš„å‚æ•°ä½œä¸ºé»˜è®¤å€¼
BLUE_HSV_LOWER = _parse_hsv_env('BLUE_HSV_LOWER', UTILS_HSV_LOWER)
BLUE_HSV_UPPER = _parse_hsv_env('BLUE_HSV_UPPER', UTILS_HSV_UPPER)

print(f"ğŸ¨ HSVé¢œè‰²è¿‡æ»¤å‚æ•° (æ¥è‡ª utils_plane.py):")
print(f"  ä¸‹é™: {BLUE_HSV_LOWER}")
print(f"  ä¸Šé™: {BLUE_HSV_UPPER}")
print(f"  æ³¨æ„: è¿™äº›å‚æ•°ä¸ utils_plane.py åŒæ­¥ï¼Œå·²ç²¾å¿ƒè°ƒæ•´")


# ========================================
# é¼ æ ‡å›è°ƒå‡½æ•°
# ========================================
def mouse_callback(event, x, y, flags, param):
    """å¤„ç†é¼ æ ‡äº‹ä»¶ï¼Œå…è®¸ç”¨æˆ·ç‚¹å‡»é€‰æ‹©æŒ‰é’®"""
    global selected_button_index, mouse_x, mouse_y, all_detections
    global current_depth_data, current_color_data, current_depth_intrin
    global selected_button_locked, is_paused, paused_detections
    global selected_box_signature, global_panel_info
    
    mouse_x, mouse_y = x, y
    
    if event == cv2.EVENT_LBUTTONDOWN:
        print(f"\n{'='*70}")
        print(f"[é¼ æ ‡ç‚¹å‡»] ä½ç½®: ({x}, {y})")
        print(f"[æ£€æµ‹çŠ¶æ€] å½“å‰æ£€æµ‹åˆ° {len(all_detections)} ä¸ªæŒ‰é’®")
        
        # ğŸ”’ çº¿ç¨‹å®‰å…¨åœ°è¯»å–æ£€æµ‹ç»“æœ
        with detection_lock:
            detections_to_check = paused_detections if is_paused else list(all_detections)
            panel_info_snapshot = global_panel_info
        
        found = False
        for idx, det in enumerate(detections_to_check):
            x1, y1, x2, y2, class_name, conf, center_3d = det
            
            print(f"  æ£€æŸ¥æŒ‰é’® #{idx}: ç±»å‹={class_name}, æ¡†=[{x1},{y1},{x2},{y2}]", end="")
            
            if x1 <= x <= x2 and y1 <= y <= y2:
                print(" â†’ âœ“ åŒ¹é…!")
                found = True
                selected_button_index = idx
                selected_button_locked = True
                
                # ğŸ”§ ä¸å†å†»ç»“æ£€æµ‹ï¼Œä¿æŒæ£€æµ‹æ¡†å®æ—¶è·Ÿéš
                global detection_frozen
                detection_frozen = False  # æ”¹ä¸ºFalseï¼Œä¿æŒæ£€æµ‹æ›´æ–°
                
                # ğŸ”§ ç«‹å³å¼ºåˆ¶åˆ·æ–°æ˜¾ç¤ºï¼ˆä¸ç­‰å¾…ä¸‹ä¸€å¸§ï¼‰
                if current_color_data is not None:
                    instant_display = visualize_detections(
                        current_color_data,
                        detections_to_check,
                        selected_button_index,
                        panel_info=panel_info_snapshot
                    )
                    cv2.imshow('detection', instant_display)
                    cv2.waitKey(1)  # ç«‹å³åˆ·æ–°
                    
                # ğŸ”Š å¯é€‰ï¼šç»ˆç«¯å“é“ƒï¼ˆæä¾›éŸ³é¢‘åé¦ˆï¼‰
                print('\a', end='', flush=True)  # ASCII Bell
                
                if center_3d is None and current_depth_data is not None:
                    center_3d, stats = extract_roi_cloud(
                        current_depth_data, 
                        current_color_data, 
                        [x1, y1, x2, y2], 
                        current_depth_intrin,
                        verbose=False
                    )
                    
                    if is_paused:
                        paused_detections[idx] = (x1, y1, x2, y2, class_name, conf, center_3d)
                    
                    # ğŸ”’ çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°all_detections
                    with detection_lock:
                        if idx < len(all_detections):
                            all_detections[idx] = (x1, y1, x2, y2, class_name, conf, center_3d)
                
                remember_selected_detection(det)
                
                print(f"\n{'='*70}")
                print(f"âœ“âœ“âœ“ å·²é€‰æ‹©æŒ‰é’® #{idx} ã€å·²é”å®šã€‘âœ“âœ“âœ“")
                print(f"  ç±»å‹: {class_name}")
                print(f"  ç½®ä¿¡åº¦: {conf:.2f}")
                print(f"  æ£€æµ‹æ¡†: [{x1}, {y1}, {x2}, {y2}]")
                print(f"  2Dä¸­å¿ƒ: ({int((x1+x2)/2)}, {int((y1+y2)/2)})")
                if center_3d is not None:
                    print(f"  3Dä½ç½®: ({center_3d[0]:.3f}, {center_3d[1]:.3f}, {center_3d[2]:.3f})")
                print(f"  æç¤º: æŒ‰ ENTER ç¡®è®¤ | æŒ‰ ESC å–æ¶ˆé€‰æ‹©")
                print(f"{'='*70}")
                break
            else:
                print(" â†’ âœ— ä¸åŒ¹é…")
        
        if not found:
            print(f"\n  âœ—âœ—âœ— ç‚¹å‡»ä½ç½® ({x}, {y}) ä¸åœ¨ä»»ä½•æŒ‰é’®å†…")
            print(f"{'='*70}")


# ========================================
# YOLO æ£€æµ‹
# ========================================
def YOLODetection(model, color_img, conf_threshold=0.5):
    """YOLO æ£€æµ‹ - å›ºå®šç¼–å·é¡ºåº"""
    results = model(color_img, verbose=False)
    target_boxes = []
    
    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            conf = float(box.conf[0])
            cls_id = int(box.cls[0])
            class_name = model.names[cls_id]
            
            if conf >= conf_threshold:
                target_boxes.append((x1, y1, x2, y2, class_name, conf))
    
    target_boxes.sort(key=lambda box: (box[1] // 100, box[0]))
    return target_boxes


def _make_signature_from_detection(det):
    """æ ¹æ®æ£€æµ‹æ¡†ç”Ÿæˆå”¯ä¸€ç­¾å"""
    x1, y1, x2, y2, class_name, *_ = det
    center = ((x1 + x2) / 2.0, (y1 + y2) / 2.0)
    return {"class": class_name, "center": center, "bbox": (x1, y1, x2, y2)}


def remember_selected_detection(det):
    """è®°å½•å½“å‰é€‰ä¸­çš„æ£€æµ‹æ¡†ç‰¹å¾"""
    global selected_box_signature
    selected_box_signature = _make_signature_from_detection(det)


def sync_selection_with_detections():
    """åœ¨æ£€æµ‹ç»“æœæ›´æ–°åï¼Œé‡æ–°å…³è”å·²é€‰ä¸­çš„æŒ‰é’®"""
    global selected_button_index, selected_box_signature

    if selected_box_signature is None or not all_detections:
        return

    best_idx = -1
    best_dist = float('inf')
    target_class = selected_box_signature["class"]
    target_center = selected_box_signature["center"]

    for idx, det in enumerate(all_detections):
        x1, y1, x2, y2, class_name, *_ = det
        if class_name != target_class:
            continue
        center = ((x1 + x2) / 2.0, (y1 + y2) / 2.0)
        dist = np.linalg.norm(np.array(center) - np.array(target_center))
        if dist < best_dist:
            best_dist = dist
            best_idx = idx

    if best_idx != -1 and best_dist < 80:
        selected_button_index = best_idx
    else:
        selected_button_index = -1
        selected_box_signature = None


# ========================================
# ğŸ”§ å¼‚æ­¥æ£€æµ‹çº¿ç¨‹
# ========================================
def async_detection_worker(model, node):
    """
    åå°æ£€æµ‹çº¿ç¨‹ï¼šä»é˜Ÿåˆ—è·å–å¸§ï¼Œæ‰§è¡ŒYOLOæ£€æµ‹ï¼Œæ›´æ–°å…¨å±€ç»“æœ
    è¿™æ ·ä¸»çº¿ç¨‹å¯ä»¥ä¿æŒ30fpså®æ—¶æ˜¾ç¤ºç”»é¢
    """
    global all_detections, detection_running, last_detect_time_ms
    global global_panel_normal, global_panel_normal_base
    global global_panel_info, panel_last_update
    
    node.get_logger().info("ğŸš€ å¼‚æ­¥æ£€æµ‹çº¿ç¨‹å·²å¯åŠ¨")
    
    while detection_running and rclpy.ok():
        try:
            # ğŸ”§ æé™ä¼˜åŒ–ï¼šæ¸…ç©ºé˜Ÿåˆ—ï¼Œåªå¤„ç†æœ€æ–°å¸§ï¼ˆé¿å…ä»»ä½•æ»åï¼‰
            frame_data = None
            while not detection_queue.empty():
                try:
                    frame_data = detection_queue.get_nowait()
                except:
                    break
            
            # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼ŒçŸ­æš‚ä¼‘çœ åç»§ç»­ï¼ˆéé˜»å¡ï¼‰
            if frame_data is None:
                time.sleep(0.01)  # 10msè½®è¯¢é—´éš”
                continue
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆNoneè¡¨ç¤ºé€€å‡ºï¼‰
            if not isinstance(frame_data, tuple):
                break
            
            color_data, depth_data, depth_intrin = frame_data
            
            # ğŸ”§ YOLOæ£€æµ‹ï¼ˆç¼©å°å›¾åƒåŠ é€Ÿï¼‰
            detect_start = time.time()
            
            # ä½¿ç”¨æœ€å¿«çš„æ’å€¼æ–¹æ³•
            color_data_small = cv2.resize(color_data, None, 
                                         fx=YOLO_SCALE_FACTOR, fy=YOLO_SCALE_FACTOR, 
                                         interpolation=cv2.INTER_NEAREST)  # æœ€å¿«æ’å€¼
            
            target_boxes_small = YOLODetection(model, color_data_small, conf_threshold=YOLO_CONF_THRESHOLD)
            detect_time_ms = (time.time() - detect_start) * 1000
            
            # ç¼©æ”¾å›åŸå§‹å°ºå¯¸
            target_boxes = []
            for x1, y1, x2, y2, class_name, conf in target_boxes_small:
                target_boxes.append((
                    int(x1 / YOLO_SCALE_FACTOR), int(y1 / YOLO_SCALE_FACTOR),
                    int(x2 / YOLO_SCALE_FACTOR), int(y2 / YOLO_SCALE_FACTOR),
                    class_name, conf
                ))
            
            # ğŸ”’ çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°æ£€æµ‹ç»“æœ
            with detection_lock:
                all_detections = []
                for box in target_boxes:
                    x1, y1, x2, y2, class_name, conf = box
                    all_detections.append((x1, y1, x2, y2, class_name, conf, None))
                
                last_detect_time_ms = detect_time_ms
                sync_selection_with_detections()
            
            # ğŸ”§ è®¡ç®—é¢æ¿æ³•å‘é‡ï¼ˆå¸¦ç¼“å­˜ï¼Œä½é¢‘æ›´æ–°ï¼‰
            # âœ… æ–°æ–¹æ¡ˆï¼šç›´æ¥ä»è“è‰²åŒºåŸŸè®¡ç®—ï¼Œä¸ä¾èµ–æŒ‰é’®æ•°é‡
            current_time = time.time()
            should_update_normal = (
                PANEL_CACHE_TIME <= 0.0 or
                (current_time - panel_last_update > PANEL_CACHE_TIME)
            )
            
            if should_update_normal:
                try:
                    panel_info = compute_panel_normal_from_blue_region(
                        depth_data=depth_data,
                        color_image=color_data,
                        depth_intrin=depth_intrin,
                        hsv_lower=BLUE_HSV_LOWER,
                        hsv_upper=BLUE_HSV_UPPER,
                        verbose=False
                    )
                    
                    if panel_info is not None:
                        normal_camera = panel_info['normal']
                        normal_base = None

                        if node.piper is not None and node.piper_arm is not None:
                            normal_base = transform_normal_camera_to_base(
                                normal_camera, node.piper, node.piper_arm
                            )
                        panel_info['normal_base'] = normal_base
                        panel_info['timestamp'] = current_time

                        with detection_lock:
                            global_panel_normal = normal_camera
                            global_panel_normal_base = normal_base
                            global_panel_info = panel_info
                            panel_last_update = current_time
                        
                        # ğŸ”§ å‘å¸ƒç›¸æœºç³»æ³•å‘é‡ï¼ˆå…¼å®¹æ—§è®¢é˜…è€…ï¼‰
                        normal_msg = Vector3()
                        normal_msg.x = float(normal_camera[0])
                        normal_msg.y = float(normal_camera[1])
                        normal_msg.z = float(normal_camera[2])
                        node.normal_pub.publish(normal_msg)

                        # ğŸ”§ é¢å¤–å‘å¸ƒåŸºåº§ç³»æ³•å‘é‡ï¼ˆè‹¥å¯è½¬æ¢ï¼‰
                        if normal_base is not None and node.normal_base_pub is not None:
                            normal_base_msg = Vector3()
                            normal_base_msg.x = float(normal_base[0])
                            normal_base_msg.y = float(normal_base[1])
                            normal_base_msg.z = float(normal_base[2])
                            node.normal_base_pub.publish(normal_base_msg)
                        
                        node.get_logger().info(
                            f"âœ“ æ³•å‘é‡(ç›¸æœºç³»): ({normal_camera[0]:+.3f}, {normal_camera[1]:+.3f}, {normal_camera[2]:+.3f})"
                            + (
                                f" â†’ (åŸºåº§ç³»): ({normal_base[0]:+.3f}, {normal_base[1]:+.3f}, {normal_base[2]:+.3f})"
                                if normal_base is not None else
                                "ï¼ˆâš ï¸ æ‰‹çœ¼ä¸å¯ç”¨ï¼Œæš‚æœªè½¬æ¢ï¼‰"
                            )
                        )
                        
                except Exception as e:
                    node.get_logger().warn(f"æ³•å‘é‡è®¡ç®—å¤±è´¥: {e}")
        
        except Exception as e:
            if detection_running:  # å¿½ç•¥æ­£å¸¸é€€å‡ºæ—¶çš„å¼‚å¸¸
                node.get_logger().error(f"âŒ æ£€æµ‹çº¿ç¨‹å¼‚å¸¸: {e}")
    
    node.get_logger().info("ğŸ›‘ å¼‚æ­¥æ£€æµ‹çº¿ç¨‹å·²åœæ­¢")


# ========================================
# å¯è§†åŒ–
# ========================================
def visualize_detections(color_img, detections, selected_idx, panel_info=None):
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
    annotated = color_img.copy()
    
    # ğŸ”§ æ£€æŸ¥é¼ æ ‡æ‚¬åœï¼ˆé¢„è§ˆæ•ˆæœï¼‰
    hover_idx = -1
    for idx, det in enumerate(detections):
        x1, y1, x2, y2, _, _, _ = det
        if x1 <= mouse_x <= x2 and y1 <= mouse_y <= y2:
            hover_idx = idx
            break
    
    for idx, det in enumerate(detections):
        x1, y1, x2, y2, class_name, conf, center_3d = det
        
        if idx == selected_idx:
            # ğŸ”§ é€‰ä¸­æ•ˆæœå¢å¼ºï¼šç»¿è‰²ç²—è¾¹æ¡† + åŠé€æ˜å¡«å……
            color = (0, 255, 0)
            thickness = 4  # æ›´ç²—
            
            # æ·»åŠ åŠé€æ˜ç»¿è‰²å¡«å……
            overlay = annotated.copy()
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
            cv2.addWeighted(overlay, 0.2, annotated, 0.8, 0, annotated)
        elif idx == hover_idx:
            # ğŸ”§ é¼ æ ‡æ‚¬åœæ•ˆæœï¼šé»„è‰²è¾¹æ¡†
            color = (0, 255, 255)
            thickness = 3
        else:
            color = (255, 0, 0)
            thickness = 2
        
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, thickness)
        
        label = f"#{idx} {class_name} {conf:.2f}"
        cv2.putText(annotated, label, (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        if center_3d is not None:
            coord_text = f"({center_3d[0]:.2f}, {center_3d[1]:.2f}, {center_3d[2]:.2f})"
            cv2.putText(annotated, coord_text, (x1, y2 + 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
    
    global current_fps, last_detect_time_ms, detection_frozen
    
    # ========================================
    # é¡¶éƒ¨ä¿¡æ¯æ ï¼ˆæ·±è‰²åŠé€æ˜èƒŒæ™¯ï¼‰
    # ========================================
    overlay = annotated.copy()
    cv2.rectangle(overlay, (0, 0), (annotated.shape[1], 140), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.5, annotated, 0.5, 0, annotated)
    
    # ç¬¬ä¸€è¡Œï¼šæ“ä½œæç¤º
    cv2.putText(annotated, "Click:Select | ESC:Cancel | ENTER:Confirm", 
                (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
    
    # ç¬¬äºŒè¡Œï¼šæ³•å‘é‡-ç›¸æœºç³»ï¼ˆåŠ¨æ€ï¼Œè“è‰²ï¼‰
    if panel_info is not None:
        normal_cam = panel_info.get('normal') if isinstance(panel_info, dict) else None
        if normal_cam is not None:
            cam_text = f"Normal_Cam: ({normal_cam[0]:+.3f}, {normal_cam[1]:+.3f}, {normal_cam[2]:+.3f})"
            cv2.putText(annotated, cam_text, (10, 55), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255, 200, 0), 2)
        
        # ç¬¬ä¸‰è¡Œï¼šæ³•å‘é‡-åŸºåº§ç³»ï¼ˆå›ºå®šï¼Œç»¿è‰²ï¼‰
        normal_base = panel_info.get('normal_base') if isinstance(panel_info, dict) else None
        if normal_base is not None:
            base_text = f"Normal_Base: ({normal_base[0]:+.3f}, {normal_base[1]:+.3f}, {normal_base[2]:+.3f})"
            cv2.putText(annotated, base_text, (10, 85), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 100), 2)
        else:
            cv2.putText(annotated, "Normal_Base: Computing...", (10, 85),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.55, (100, 100, 100), 2)
        
        # ç¬¬å››è¡Œï¼šé¢æ¿æ·±åº¦ + æ›´æ–°å»¶è¿Ÿ
        panel_depth = panel_info.get('median_depth') if isinstance(panel_info, dict) else None
        timestamp = panel_info.get('timestamp') if isinstance(panel_info, dict) else None
        
        info_parts = []
        if panel_depth is not None:
            info_parts.append(f"Depth:{panel_depth*100:.1f}cm")
        if timestamp is not None:
            age_ms = max(0.0, (time.time() - timestamp) * 1000.0)
            info_parts.append(f"Age:{age_ms:.0f}ms")
        
        if info_parts:
            info_text = " | ".join(info_parts)
            cv2.putText(annotated, info_text, (10, 115),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180, 180, 180), 1)
    else:
        cv2.putText(annotated, "Normal: Waiting for detection...", (10, 55),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.55, (100, 100, 100), 2)
    
    # ========================================
    # å³ä¸Šè§’æ€§èƒ½ä¿¡æ¯ï¼ˆç´§å‡‘å¸ƒå±€ï¼‰
    # ========================================
    right_x = annotated.shape[1] - 150
    cv2.putText(annotated, f"FPS:{current_fps:.0f}", (right_x, 25),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    if last_detect_time_ms > 0:
        cv2.putText(annotated, f"Det:{last_detect_time_ms:.0f}ms", (right_x, 55),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 128, 0), 2)
    
    cv2.putText(annotated, "LIVE", (right_x, 85),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
    
    # ğŸ”§ é¼ æ ‡ä½ç½®æŒ‡ç¤ºï¼ˆåº•éƒ¨çŠ¶æ€æ ï¼‰
    if 0 <= mouse_x < annotated.shape[1] and 0 <= mouse_y < annotated.shape[0]:
        # åº•éƒ¨çŠ¶æ€æ 
        cv2.rectangle(annotated, (0, annotated.shape[0] - 25), 
                     (annotated.shape[1], annotated.shape[0]), (0, 0, 0), -1)
        cv2.putText(annotated, f"Mouse: ({mouse_x}, {mouse_y})", 
                   (10, annotated.shape[0] - 8),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    return annotated


# ========================================
# ROI ç‚¹äº‘æå–ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
# ========================================
def extract_roi_cloud(depth_data, color_data, bbox, depth_intrin, verbose=False):
    """ä»æ·±åº¦å›¾ä¸­æå– ROI åŒºåŸŸçš„ 3D ä¸­å¿ƒç‚¹"""
    x1, y1, x2, y2 = bbox
    
    center_u = int((x1 + x2) / 2)
    center_v = int((y1 + y2) / 2)
    
    fx, fy = depth_intrin.fx, depth_intrin.fy
    cx, cy = depth_intrin.ppx, depth_intrin.ppy
    
    # ROIç»Ÿè®¡
    roi_depth = depth_data[y1:y2, x1:x2]
    valid_roi_depths = roi_depth[roi_depth > 0]
    
    point_cloud_stats = {
        'bbox_size': (x2-x1, y2-y1),
        'valid_count': len(valid_roi_depths),
        'coverage': 100 * len(valid_roi_depths) / roi_depth.size if roi_depth.size > 0 else 0,
        'depth_mean': valid_roi_depths.mean() if len(valid_roi_depths) > 0 else 0,
        'depth_std': valid_roi_depths.std() if len(valid_roi_depths) > 0 else 0,
    }
    
    # æ™ºèƒ½é‡‡æ ·ï¼š5x5çª—å£ + ä¸­ä½æ•°è¿‡æ»¤
    sample_size = 5
    u_min = max(0, center_u - sample_size)
    u_max = min(depth_data.shape[1], center_u + sample_size)
    v_min = max(0, center_v - sample_size)
    v_max = min(depth_data.shape[0], center_v + sample_size)
    
    depth_window = depth_data[v_min:v_max, u_min:u_max]
    valid_depths = depth_window[depth_window > 0]
    
    if len(valid_depths) == 0:
        return None, point_cloud_stats
    
    # ä½¿ç”¨ä¸­ä½æ•° + ç¦»ç¾¤ç‚¹è¿‡æ»¤
    depth_median = np.median(valid_depths)
    depth_std = np.std(valid_depths)
    
    if depth_std > 200:
        lower_bound = depth_median - 1.5 * depth_std
        upper_bound = depth_median + 1.5 * depth_std
        filtered_depths = valid_depths[(valid_depths >= lower_bound) & (valid_depths <= upper_bound)]
        if len(filtered_depths) > 0:
            valid_depths = filtered_depths
            depth_median = np.median(valid_depths)
    
    depth_value = depth_median
    depth_m = depth_value / 1000.0
    
    if depth_m < 0.15 or depth_m > 1.5:
        return None, point_cloud_stats
    
    x = (center_u - cx) * depth_m / fx
    y = (center_v - cy) * depth_m / fy
    z = depth_m
    
    return [x, y, z], point_cloud_stats


# ========================================
# åæ ‡è½¬æ¢ï¼šç›¸æœº â†’ åŸºåº§
# ========================================
def transform_normal_camera_to_base(normal_camera, piper, piper_arm):
    """
    å°†ç›¸æœºåæ ‡ç³»çš„æ³•å‘é‡è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
    
    æ³¨æ„ï¼šæ³•å‘é‡æ˜¯æ–¹å‘å‘é‡ï¼Œåªéœ€è¦æ—‹è½¬å˜æ¢ï¼ˆä¸éœ€è¦å¹³ç§»ï¼‰
    
    å‚æ•°:
        normal_camera: ç›¸æœºç³»æ³•å‘é‡ [nx, ny, nz]
        piper: æœºæ¢°è‡‚æ¥å£
        piper_arm: æœºæ¢°è‡‚è¿åŠ¨å­¦å¯¹è±¡
    
    è¿”å›:
        åŸºåº§ç³»æ³•å‘é‡ [nx', ny', nz'] æˆ– None
    """
    try:
        # è·å–å½“å‰å…³èŠ‚è§’åº¦
        msg = piper.GetArmJointMsgs()
        current_joints = [
            msg.joint_state.joint_1 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_2 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_3 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_4 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_5 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_6 * 1e-3 * PI / 180.0,
        ]
        
        # æ­£è¿åŠ¨å­¦ï¼šè·å– link6â†’base çš„å˜æ¢çŸ©é˜µï¼ˆ4x4é½æ¬¡å˜æ¢ï¼‰
        base_T_link6 = piper_arm.forward_kinematics(current_joints)
        
        # æå–æ—‹è½¬çŸ©é˜µï¼ˆlink6â†’baseï¼‰
        # æ³¨æ„ï¼šbase_T_link6 è¡¨ç¤ºå°† link6ç³»çš„ç‚¹/å‘é‡ è½¬æ¢åˆ° baseç³»
        R_link6_to_base = base_T_link6[:3, :3]
        
        # æ‰‹çœ¼æ ‡å®šï¼šè·å– cameraâ†’link6 çš„å›ºå®šå˜æ¢
        link6_T_camera = np.eye(4)
        link6_T_camera[:3, :3] = quaternion_to_rotation_matrix(piper_arm.link6_q_camera)
        link6_T_camera[:3, 3] = piper_arm.link6_t_camera
        
        # æå–æ—‹è½¬çŸ©é˜µï¼ˆcameraâ†’link6ï¼‰
        R_camera_to_link6 = link6_T_camera[:3, :3]
        
        # ç»„åˆæ—‹è½¬çŸ©é˜µï¼šcamera â†’ link6 â†’ base
        # è½¬æ¢é“¾ï¼šç›¸æœºç³» â†’ link6ç³» â†’ åŸºåº§ç³»
        R_camera_to_base = R_link6_to_base @ R_camera_to_link6
        
        # æ—‹è½¬æ³•å‘é‡ï¼ˆæ³•å‘é‡åªéœ€è¦æ—‹è½¬ï¼Œä¸éœ€è¦å¹³ç§»ï¼‰
        normal_base = R_camera_to_base @ np.array(normal_camera)
        
        # å½’ä¸€åŒ–
        normal_base = normal_base / np.linalg.norm(normal_base)
        
        return normal_base.tolist()
    
    except Exception as e:
        print(f"âŒ æ³•å‘é‡åæ ‡è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def transform_camera_to_base(button_camera, piper, piper_arm):
    """å°†ç›¸æœºåæ ‡ç³»çš„æŒ‰é’®ä½ç½®è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»"""
    try:
        msg = piper.GetArmJointMsgs()
        current_joints = [
            msg.joint_state.joint_1 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_2 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_3 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_4 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_5 * 1e-3 * PI / 180.0,
            msg.joint_state.joint_6 * 1e-3 * PI / 180.0,
        ]
        
        base_T_link6 = piper_arm.forward_kinematics(current_joints)
        
        link6_T_cam = np.eye(4)
        link6_T_cam[:3, :3] = quaternion_to_rotation_matrix(piper_arm.link6_q_camera)
        link6_T_cam[:3, 3] = piper_arm.link6_t_camera
        
        button_cam_h = np.array([button_camera[0], button_camera[1], button_camera[2], 1.0])
        button_base = base_T_link6 @ link6_T_cam @ button_cam_h
        
        return button_base[:3]
    except Exception as e:
        print(f"  âš ï¸  åæ ‡è½¬æ¢å¤±è´¥: {e}")
        return None


# ========================================
# ROS2 èŠ‚ç‚¹ï¼ˆä»…ç”¨äºå‘å¸ƒç»“æœï¼‰
# ========================================
class ButtonDetectorNode(Node):
    """æŒ‰é’®æ£€æµ‹èŠ‚ç‚¹ - ç›´æ¥è¯»å–ç›¸æœºç‰ˆæœ¬"""
    
    def __init__(self):
        super().__init__('button_detector_ros2_direct')
        
        self.get_logger().info("="*70)
        self.get_logger().info("äº¤äº’å¼æŒ‰é’®æ£€æµ‹å™¨ - ROS2 ç›´æ¥è¯»å–ç‰ˆæœ¬ï¼ˆé«˜æ€§èƒ½ï¼‰")
        self.get_logger().info("="*70)
        
        # åˆå§‹åŒ– Piper SDK
        try:
            self.piper = C_PiperInterface_V2("can0")
            self.piper.ConnectPort()
            self.piper_arm = PiperArm()
            self.get_logger().info("âœ“ Piper SDK åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.get_logger().warn(f"âš ï¸  Piper SDK åˆå§‹åŒ–å¤±è´¥: {e}")
            self.piper = None
            self.piper_arm = None
        
        # åŠ è½½ YOLO æ¨¡å‹
        self.model = YOLO('yolo_button.pt')
        self.get_logger().info("âœ“ YOLO æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºå‘å¸ƒå™¨
        self.point_pub = self.create_publisher(PointStamped, '/object_point', 10)
        self.type_pub = self.create_publisher(String, '/button_type', 10)
        self.marker_pub = self.create_publisher(Marker, '/object_center_marker', 10)
        self.normal_pub = self.create_publisher(Vector3, '/button_normal', 10)  # ç›¸æœºç³»æ³•å‘é‡
        self.normal_base_pub = self.create_publisher(Vector3, '/button_normal_base', 10)  # åŸºåº§ç³»æ³•å‘é‡
        
        self.get_logger().info("âœ“ ROS2å‘å¸ƒå™¨å·²åˆ›å»ºï¼ˆåŒ…æ‹¬æ³•å‘é‡è¯é¢˜ï¼‰")
        self.get_logger().info("="*70)
    
    def publish_result(self, center_3d, class_name):
        """å‘å¸ƒæ£€æµ‹ç»“æœåˆ°ROS2è¯é¢˜"""
        # å‘å¸ƒç‚¹åæ ‡
        point_msg = PointStamped()
        point_msg.header.stamp = self.get_clock().now().to_msg()
        point_msg.header.frame_id = "camera"
        point_msg.point.x = center_3d[0]
        point_msg.point.y = center_3d[1]
        point_msg.point.z = center_3d[2]
        self.point_pub.publish(point_msg)
        
        # å‘å¸ƒæŒ‰é’®ç±»å‹
        type_msg = String()
        type_msg.data = class_name
        self.type_pub.publish(type_msg)
        
        # å‘å¸ƒå¯è§†åŒ–Marker
        marker = Marker()
        marker.header.frame_id = "camera"
        marker.header.stamp = self.get_clock().now().to_msg()
        marker.ns = "button_center"
        marker.id = 0
        marker.type = Marker.SPHERE
        marker.action = Marker.ADD
        marker.pose.position.x = center_3d[0]
        marker.pose.position.y = center_3d[1]
        marker.pose.position.z = center_3d[2]
        marker.scale.x = 0.02
        marker.scale.y = 0.02
        marker.scale.z = 0.02
        marker.color.r = 1.0
        marker.color.g = 0.0
        marker.color.b = 0.0
        marker.color.a = 1.0
        self.marker_pub.publish(marker)
    
    def publish_result_with_normal(self, center_3d, class_name, normal_vector, in_camera_frame=True):
        """
        å‘å¸ƒæ£€æµ‹ç»“æœåˆ°ROS2è¯é¢˜ï¼ˆåŒ…å«æ³•å‘é‡ï¼‰
        
        å‚æ•°:
            center_3d: æŒ‰é’®ä¸­å¿ƒ3Dåæ ‡
            class_name: æŒ‰é’®ç±»å‹
            normal_vector: é¢æ¿æ³•å‘é‡
            in_camera_frame: æ³•å‘é‡æ˜¯å¦åœ¨ç›¸æœºåæ ‡ç³»ï¼ˆTrueï¼‰è¿˜æ˜¯åŸºåº§åæ ‡ç³»ï¼ˆFalseï¼‰
        """
        # å‘å¸ƒåŸºæœ¬ä¿¡æ¯
        self.publish_result(center_3d, class_name)
        
        # å‘å¸ƒæ³•å‘é‡ï¼ˆæ³¨æ„ï¼šä½¿ç”¨Stampedæ¶ˆæ¯å¯ä»¥æ ‡è®°åæ ‡ç³»ï¼‰
        # ä½†Vector3æ²¡æœ‰headerï¼Œæ‰€ä»¥é€šè¿‡æ—¥å¿—è¯´æ˜
        normal_msg = Vector3()
        normal_msg.x = float(normal_vector[0])
        normal_msg.y = float(normal_vector[1])
        normal_msg.z = float(normal_vector[2])
        self.normal_pub.publish(normal_msg)
        
        if not in_camera_frame and self.normal_base_pub is not None:
            normal_base_msg = Vector3()
            normal_base_msg.x = normal_msg.x
            normal_base_msg.y = normal_msg.y
            normal_base_msg.z = normal_msg.z
            self.normal_base_pub.publish(normal_base_msg)
        
        frame_info = "ç›¸æœºåæ ‡ç³»" if in_camera_frame else "åŸºåº§åæ ‡ç³»"
        self.get_logger().info(f"  âœ“ æ³•å‘é‡å·²å‘å¸ƒ ({frame_info}): ({normal_vector[0]:.4f}, {normal_vector[1]:.4f}, {normal_vector[2]:.4f})")
        
        # å‘å¸ƒæ³•å‘é‡å¯è§†åŒ–ï¼ˆç®­å¤´ï¼‰
        arrow_marker = Marker()
        # æ ¹æ®æ³•å‘é‡åæ ‡ç³»è®¾ç½®frame_id
        arrow_marker.header.frame_id = "camera" if in_camera_frame else "base_link"
        arrow_marker.header.stamp = self.get_clock().now().to_msg()
        arrow_marker.ns = "panel_normal"
        arrow_marker.id = 1
        arrow_marker.type = Marker.ARROW
        arrow_marker.action = Marker.ADD
        
        # ç®­å¤´èµ·ç‚¹ï¼šæŒ‰é’®ä¸­å¿ƒ
        arrow_marker.points = []
        from geometry_msgs.msg import Point
        start_point = Point()
        start_point.x = center_3d[0]
        start_point.y = center_3d[1]
        start_point.z = center_3d[2]
        arrow_marker.points.append(start_point)
        
        # ç®­å¤´ç»ˆç‚¹ï¼šæ²¿æ³•å‘é‡å»¶ä¼¸10cm
        # å¦‚æœåœ¨ç›¸æœºç³»ï¼Œæ³•å‘é‡æŒ‡å‘ç›¸æœºï¼ˆå‡å»ï¼‰
        # å¦‚æœåœ¨åŸºåº§ç³»ï¼Œæ³•å‘é‡æŒ‡å‘å¤–ä¾§ï¼ˆåŠ ä¸Šï¼‰
        direction = -1.0 if in_camera_frame else 1.0
        end_point = Point()
        end_point.x = center_3d[0] + direction * normal_vector[0] * 0.1
        end_point.y = center_3d[1] + direction * normal_vector[1] * 0.1
        end_point.z = center_3d[2] + direction * normal_vector[2] * 0.1
        arrow_marker.points.append(end_point)
        
        # ç®­å¤´æ ·å¼
        arrow_marker.scale.x = 0.005  # ç®­å¤´è½´ç›´å¾„
        arrow_marker.scale.y = 0.01   # ç®­å¤´å¤´éƒ¨ç›´å¾„
        arrow_marker.scale.z = 0.01   # ç®­å¤´å¤´éƒ¨é•¿åº¦
        arrow_marker.color.r = 0.0
        arrow_marker.color.g = 1.0
        arrow_marker.color.b = 1.0
        arrow_marker.color.a = 1.0
        self.marker_pub.publish(arrow_marker)


def main(args=None):
    rclpy.init(args=args)
    
    node = ButtonDetectorNode()
    
    # ========================================
    # âœ… å…³é”®ä¿®å¤ï¼šå¯åŠ¨ROS2ç‹¬ç«‹spinçº¿ç¨‹
    # ========================================
    ros_spin_running = threading.Event()
    ros_spin_running.set()
    
    def ros_spin_thread():
        """ROS2äº‹ä»¶å¾ªç¯ç‹¬ç«‹çº¿ç¨‹ï¼Œé¿å…è¢«OpenCVé˜»å¡"""
        try:
            node.get_logger().info("ğŸ”„ ROS2 spinçº¿ç¨‹å·²å¯åŠ¨ï¼ˆç‹¬ç«‹è¿è¡Œï¼‰")
            while ros_spin_running.is_set() and rclpy.ok():
                rclpy.spin_once(node, timeout_sec=0.01)
        except Exception as e:
            node.get_logger().error(f"âŒ ROS2 spinçº¿ç¨‹å¼‚å¸¸: {e}")
    
    spin_thread = threading.Thread(target=ros_spin_thread, daemon=True)
    spin_thread.start()
    node.get_logger().info("âœ… ROS2æ¶ˆæ¯å¤„ç†å·²ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¼šè¢«ç›¸æœº/OpenCVé˜»å¡")
    # ========================================
    
    # é…ç½® RealSenseï¼ˆç›´æ¥è¯»å–ï¼‰
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    
    profile = pipeline.start(config)
    align = rs.align(rs.stream.color)
    
    node.get_logger().info("âœ“ RealSenseç›¸æœºå¯åŠ¨æˆåŠŸï¼ˆç›´æ¥è¯»å–æ¨¡å¼ï¼‰")
    
    # åˆ›å»º OpenCV çª—å£
    cv2.namedWindow('detection', cv2.WINDOW_AUTOSIZE)
    cv2.setMouseCallback('detection', mouse_callback, node)
    
    global all_detections, selected_button_index, selected_button_locked
    global current_depth_data, current_color_data, current_depth_intrin
    global frame_counter, last_fps_time, fps_counter, current_fps
    global is_paused, paused_frame, paused_detections
    global selected_box_signature
    global global_panel_normal, global_panel_normal_base
    global global_panel_info, panel_last_update
    global detection_running, detection_frozen
    
    # ğŸš€ å¯åŠ¨å¼‚æ­¥æ£€æµ‹çº¿ç¨‹
    detection_thread = threading.Thread(
        target=async_detection_worker, 
        args=(node.model, node), 
        daemon=True
    )
    detection_thread.start()
    node.get_logger().info("âœ… å¼‚æ­¥æ£€æµ‹çº¿ç¨‹å·²å¯åŠ¨ï¼Œç”»é¢å°†ä¿æŒå®æ—¶åˆ·æ–°")
    
    try:
        node.get_logger().info("âœ“ å¼€å§‹æ£€æµ‹...")
        
        while rclpy.ok():
            # FPSç»Ÿè®¡
            fps_counter += 1
            current_time = time.time()
            if current_time - last_fps_time >= 1.0:
                current_fps = fps_counter / (current_time - last_fps_time)
                fps_counter = 0
                last_fps_time = current_time
            
            # è·å–ç›¸æœºå¸§ï¼ˆç›´æ¥è¯»å–ï¼Œæ— å»¶è¿Ÿï¼‰
            frames = pipeline.wait_for_frames()
            aligned_frames = align.process(frames)
            
            aligned_depth = aligned_frames.get_depth_frame()
            color_frame = aligned_frames.get_color_frame()
            
            if not (aligned_depth and color_frame):
                continue
            
            depth_data = np.asanyarray(aligned_depth.get_data())
            color_data = np.asanyarray(color_frame.get_data())
            depth_intrin = aligned_depth.profile.as_video_stream_profile().intrinsics
            
            current_depth_data = depth_data
            current_color_data = color_data
            current_depth_intrin = depth_intrin
            
            # æš‚åœæ¨¡å¼
            if is_paused:
                if paused_frame is not None:
                    with detection_lock:
                        paused_panel_info = global_panel_info
                    display_img = visualize_detections(
                        paused_frame,
                        paused_detections,
                        selected_button_index,
                        panel_info=paused_panel_info
                    )
                    cv2.putText(display_img, "PAUSED - Click to Select", (10, 100), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
                    cv2.imshow('detection', display_img)
                
                key = cv2.waitKey(1) & 0xFF
                if key == 32:  # SPACE
                    is_paused = False
                    node.get_logger().info("â–¶ï¸  ç”»é¢ç»§ç»­")
                    paused_frame = None
                    paused_detections = []
                elif key == 27:  # ESC
                    selected_button_index = -1
                    selected_button_locked = False
                    selected_box_signature = None
                elif key == ord('q'):
                    break
                continue
            
            # ğŸ”§ å¼‚æ­¥æ£€æµ‹ï¼šå°†å¸§å‘é€åˆ°æ£€æµ‹é˜Ÿåˆ—ï¼ˆéé˜»å¡ï¼‰
            frame_counter += 1
            should_detect = (frame_counter % (DETECTION_SKIP_FRAMES + 1) == 0)
            
            if should_detect:
                # å°è¯•å°†å¸§æ”¾å…¥é˜Ÿåˆ—ï¼ˆä¸é˜»å¡ï¼Œå¦‚æœé˜Ÿåˆ—æ»¡äº†å°±è·³è¿‡ï¼‰
                try:
                    detection_queue.put_nowait((color_data.copy(), depth_data.copy(), depth_intrin))
                except:
                    pass  # é˜Ÿåˆ—æ»¡äº†ï¼Œè·³è¿‡æœ¬å¸§æ£€æµ‹
            
            # ğŸ”§ æ˜¾ç¤ºï¼ˆé«˜ä¼˜å…ˆçº§ï¼Œæ¯å¸§éƒ½åˆ·æ–°ï¼‰- ä½¿ç”¨é”ä¿æŠ¤è¯»å–
            with detection_lock:
                current_detections = list(all_detections)
                current_panel_info = global_panel_info
            
            display_img = visualize_detections(
                color_data,
                current_detections,
                selected_button_index,
                panel_info=current_panel_info
            )
            
            # ğŸ”§ å åŠ é¢æ¿æ³•å‘é‡å¯è§†åŒ–
            if current_panel_info is not None and len(current_detections) >= 2:
                display_img = visualize_panel_normal(
                    display_img, 
                    current_detections, 
                    current_panel_info,
                    show_rings=False  # ä¸æ˜¾ç¤ºç¯å½¢åŒºåŸŸï¼Œé¿å…ç”»é¢æ··ä¹±
                )
            
            cv2.imshow('detection', display_img)
            
            # é”®ç›˜å¤„ç†ï¼ˆä½¿ç”¨æ›´çŸ­çš„ç­‰å¾…æ—¶é—´æé«˜å“åº”é€Ÿåº¦ï¼‰
            wait_time = max(1, int(1000 / UI_REFRESH_RATE))  # 30Hz = 33ms
            key = cv2.waitKey(wait_time) & 0xFF
            
            if key == 32:  # SPACE
                is_paused = not is_paused
                if is_paused:
                    node.get_logger().info("â¸ï¸  ç”»é¢å·²æš‚åœ")
                    paused_frame = color_data.copy()
                    paused_detections = list(all_detections)
            
            elif key == 27:  # ESC
                node.get_logger().info("âœ— å·²å–æ¶ˆé€‰æ‹©ï¼Œè§£é™¤é”å®šï¼Œæ¢å¤æ£€æµ‹")
                selected_button_index = -1
                selected_button_locked = False
                selected_box_signature = None
                detection_frozen = False  # ğŸ”§ æ¢å¤æ£€æµ‹
            
            if key == 13:  # ENTER
                # ğŸ”’ çº¿ç¨‹å®‰å…¨åœ°è¯»å–æ£€æµ‹ç»“æœ
                with detection_lock:
                    detections_to_use = paused_detections if is_paused else list(all_detections)
                    current_global_normal = global_panel_normal
                    current_global_normal_base = global_panel_normal_base
                
                if selected_button_index >= 0 and selected_button_index < len(detections_to_use):
                    det = detections_to_use[selected_button_index]
                    x1, y1, x2, y2, class_name, conf, center_3d = det
                    
                    if center_3d is None:
                        center_3d, stats = extract_roi_cloud(
                            current_depth_data, current_color_data, 
                            [x1, y1, x2, y2], 
                            current_depth_intrin,
                            verbose=True
                        )
                    
                    if center_3d is not None:
                        node.get_logger().info(f"\n{'='*70}")
                        node.get_logger().info(f"âœ“ ç¡®è®¤é€‰æ‹©:")
                        node.get_logger().info(f"  æŒ‰é’®ç±»å‹: {class_name}")
                        node.get_logger().info(f"  ç›¸æœºåæ ‡: ({center_3d[0]:.3f}, {center_3d[1]:.3f}, {center_3d[2]:.3f}) m")
                        
                        # ğŸ”§ ä½¿ç”¨å…¨å±€æ³•å‘é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        use_normal = None
                        use_normal_base = None
                        if current_global_normal is not None:
                            use_normal = current_global_normal
                            use_normal_base = current_global_normal_base
                            node.get_logger().info(f"  âœ“ ä½¿ç”¨ç¼“å­˜çš„é¢æ¿æ³•å‘é‡")
                        else:
                            # å›é€€ï¼šå°è¯•å±€éƒ¨è®¡ç®—ï¼ˆä»…å½“å‰æŒ‰é’®ï¼‰
                            node.get_logger().info(f"  âš ï¸  æ— å…¨å±€æ³•å‘é‡ï¼Œå°è¯•å±€éƒ¨è®¡ç®—...")
                            try:
                                local_panel_info = compute_robust_panel_normal(
                                    [det],  # åªç”¨å½“å‰æŒ‰é’®
                                    current_depth_data,
                                    current_depth_intrin,
                                    expand_ratio=0.3,  # å±€éƒ¨è®¡ç®—æ—¶æ‰©å±•ç¨å¤§
                                    min_buttons=1,
                                    verbose=True,
                                    color_image=current_color_data,  # ğŸ”§ ä¼ å…¥å½©è‰²å›¾åƒ
                                    use_color_filter=True,  # ğŸ”§ å¯ç”¨è“è‰²é¢æ¿è¿‡æ»¤
                                    hsv_lower=BLUE_HSV_LOWER,  # ğŸ¨ HSVä¸‹é™
                                    hsv_upper=BLUE_HSV_UPPER   # ğŸ¨ HSVä¸Šé™
                                )
                                if local_panel_info is not None:
                                    use_normal = local_panel_info['normal']
                                    node.get_logger().info(f"  âœ“ å±€éƒ¨æ³•å‘é‡è®¡ç®—æˆåŠŸ")
                            except Exception as e:
                                node.get_logger().warn(f"  âš ï¸  å±€éƒ¨æ³•å‘é‡è®¡ç®—å¤±è´¥: {e}")
                        
                        if use_normal is not None:
                            node.get_logger().info(f"  é¢æ¿æ³•å‘é‡(ç›¸æœºç³»): ({use_normal[0]:.4f}, {use_normal[1]:.4f}, {use_normal[2]:.4f})")
                        else:
                            node.get_logger().warn(f"  âš ï¸  æ— æ³•è®¡ç®—æ³•å‘é‡ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚ç›´æ–¹å‘")
                            use_normal = np.array([0.0, 0.0, -1.0])  # é»˜è®¤ï¼šå‚ç›´äºç›¸æœº
                        
                        # ğŸ”§ è½¬æ¢æ³•å‘é‡åˆ°åŸºåº§åæ ‡ç³»ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
                        normal_in_camera_frame = True  # åˆå§‹ï¼šç›¸æœºç³»
                        if use_normal_base is not None:
                            use_normal = use_normal_base
                            normal_in_camera_frame = False
                            node.get_logger().info(
                                f"  é¢æ¿æ³•å‘é‡(åŸºåº§ç³»-ç¼“å­˜): ({use_normal_base[0]:.4f}, {use_normal_base[1]:.4f}, {use_normal_base[2]:.4f})"
                            )
                        if node.piper is not None:
                            base_3d = transform_camera_to_base(center_3d, node.piper, node.piper_arm)
                            if base_3d is not None:
                                node.get_logger().info(f"  æŒ‰é’®ä½ç½®(åŸºåº§ç³»): ({base_3d[0]:.3f}, {base_3d[1]:.3f}, {base_3d[2]:.3f}) m")
                            
                            # è½¬æ¢æ³•å‘é‡åˆ°åŸºåº§ç³»
                            if use_normal_base is None:
                                converted_normal = transform_normal_camera_to_base(use_normal, node.piper, node.piper_arm)
                                if converted_normal is not None:
                                    use_normal = converted_normal
                                    use_normal_base = converted_normal
                                    node.get_logger().info(f"  é¢æ¿æ³•å‘é‡(åŸºåº§ç³»): ({converted_normal[0]:.4f}, {converted_normal[1]:.4f}, {converted_normal[2]:.4f})")
                                    normal_in_camera_frame = False
                                else:
                                    node.get_logger().warn(f"  âš ï¸  æ³•å‘é‡åæ ‡è½¬æ¢å¤±è´¥ï¼Œå°†ä½¿ç”¨ç›¸æœºç³»æ³•å‘é‡")
                        
                        node.get_logger().info(f"{'='*70}")
                        
                        # å‘å¸ƒåˆ°ROS2ï¼ˆåŒ…å«æ³•å‘é‡ï¼‰
                        node.publish_result_with_normal(center_3d, class_name, use_normal, in_camera_frame=normal_in_camera_frame)
                        node.get_logger().info("âœ“ å·²å‘å¸ƒåˆ° ROS2 è¯é¢˜ï¼ˆåŒ…å«æ³•å‘é‡ï¼‰")
                        
                        selected_button_index = -1
                        selected_button_locked = False
                        selected_box_signature = None
                        detection_frozen = False  # ğŸ”§ æ¢å¤æ£€æµ‹
            
            elif key == ord('q'):
                break
            
            # ========================================
            # âŒ åˆ é™¤ï¼šä¸å†éœ€è¦spin_onceï¼ˆç‹¬ç«‹çº¿ç¨‹å·²å¤„ç†ï¼‰
            # rclpy.spin_once(node, timeout_sec=0)
            # ========================================
    
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­")
    finally:
        # ğŸ”§ åœæ­¢å¼‚æ­¥æ£€æµ‹çº¿ç¨‹
        detection_running = False
        detection_queue.put(None)  # å‘é€åœæ­¢ä¿¡å·
        detection_thread.join(timeout=2.0)
        node.get_logger().info("âœ“ å¼‚æ­¥æ£€æµ‹çº¿ç¨‹å·²åœæ­¢")
        
        # åœæ­¢ROS2 spinçº¿ç¨‹
        ros_spin_running.clear()
        spin_thread.join(timeout=2.0)
        
        pipeline.stop()
        cv2.destroyAllWindows()
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
